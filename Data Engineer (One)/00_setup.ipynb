{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5f3388-d634-4d42-8fe4-80d846028294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training Environment Setup\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This setup script creates **isolated training environments** for each participant:\n",
    "\n",
    "1. **Gets current user** from Databricks session\n",
    "2. **Creates personal catalog** (`training_<username>`)\n",
    "3. **Creates medallion schemas** (bronze, silver, gold)\n",
    "4. **Sets up data paths** for Unity Catalog Volumes\n",
    "\n",
    "## Why Per-User Isolation?\n",
    "\n",
    "| Approach | Pros | Cons | When to Use |\n",
    "|----------|------|------|-------------|\n",
    "| **Catalog per user** | Full isolation, realistic production setup | Requires CREATE CATALOG permission | Training, sandbox environments |\n",
    "| **Schema per user** | Simpler permissions | Shared catalog, potential conflicts | Quick demos, limited permissions |\n",
    "| **Shared everything** | Easiest setup | No isolation, data conflicts | Single-user demos only |\n",
    "\n",
    "**We use Catalog isolation** because:\n",
    "- Mimics production environments (each team/domain has own catalog)\n",
    "- Demonstrates Unity Catalog governance features\n",
    "- Prevents conflicts between participants\n",
    "- Each user can DROP/CREATE without affecting others\n",
    "\n",
    "## Cost Considerations\n",
    "\n",
    "- **Storage**: Each catalog has separate storage location (managed by Unity Catalog)\n",
    "- **Compute**: Shared cluster, no additional cost per catalog\n",
    "- **Cleanup**: Catalogs can be dropped after training to free storage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0c37e9f-38e8-4fd2-bf40-bdc57dbd0a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: Get Current User (Dynamic)\n",
    "# =============================================================================\n",
    "# This automatically gets the logged-in user from Databricks session\n",
    "raw_user = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "print(f\"Current user: {raw_user}\")\n",
    "\n",
    "# Create a clean slug from email (e.g., \"jan.kowalski@company.com\" -> \"jan_kowalski\")\n",
    "user_slug = re.sub(r'[^a-zA-Z0-9]', '_', raw_user.split('@')[0]).lower()\n",
    "print(f\"User slug: {user_slug}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Variables\n",
    "\n",
    "After running this setup, the following variables are available in all notebooks:\n",
    "\n",
    "| Variable | Description | Example |\n",
    "|----------|-------------|---------|\n",
    "| `CATALOG` | User's personal catalog | `training_jan_kowalski` |\n",
    "| `BRONZE_SCHEMA` | Raw data layer | `bronze` |\n",
    "| `SILVER_SCHEMA` | Cleaned/validated data | `silver` |\n",
    "| `GOLD_SCHEMA` | Business-ready aggregates | `gold` |\n",
    "| `DATASET_BASE_PATH` | Path to source data files | `/Volumes/.../datasets` |\n",
    "| `user_slug` | Sanitized username | `jan_kowalski` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Define Catalog and Schema Names\n",
    "# =============================================================================\n",
    "# Each user gets their own catalog with medallion architecture schemas\n",
    "\n",
    "CATALOG = f\"training_{user_slug}\"\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "SILVER_SCHEMA = \"silver\"\n",
    "GOLD_SCHEMA = \"gold\"\n",
    "\n",
    "# Shared data source (read-only Volume with training datasets)\n",
    "# This Volume is pre-created by trainer and shared with all participants\n",
    "SHARED_DATA_CATALOG = \"training_shared\"\n",
    "DATASET_BASE_PATH = f\"/Volumes/{SHARED_DATA_CATALOG}/datasets/files\"\n",
    "\n",
    "print(f\"User catalog: {CATALOG}\")\n",
    "print(f\"Schemas: {BRONZE_SCHEMA}, {SILVER_SCHEMA}, {GOLD_SCHEMA}\")\n",
    "print(f\"Source data: {DATASET_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create User Catalog and Schemas\n",
    "\n",
    "**What happens below:**\n",
    "1. Creates catalog if not exists (requires CREATE CATALOG permission)\n",
    "2. Creates bronze/silver/gold schemas\n",
    "3. Sets the catalog as default context\n",
    "\n",
    "**If you get permission error:**\n",
    "- Ask your Databricks admin to grant: `GRANT CREATE CATALOG ON METASTORE TO <user>`\n",
    "- Or use a pre-created catalog (change CATALOG variable above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Create Catalog (if not exists)\n",
    "# =============================================================================\n",
    "try:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "    print(f\"Catalog '{CATALOG}' ready\")\n",
    "except Exception as e:\n",
    "    if \"PERMISSION_DENIED\" in str(e) or \"does not have\" in str(e):\n",
    "        print(f\"ERROR: No permission to create catalog.\")\n",
    "        print(f\"Ask admin to run: GRANT CREATE CATALOG ON METASTORE TO `{raw_user}`\")\n",
    "        print(f\"Or use existing catalog by changing CATALOG variable above.\")\n",
    "        raise\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: Create Medallion Schemas\n",
    "# =============================================================================\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "for schema_name in [BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    print(f\"Schema '{CATALOG}.{schema_name}' ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: Verify Setup\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING ENVIRONMENT READY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"User:              {raw_user}\")\n",
    "print(f\"Catalog:           {CATALOG}\")\n",
    "print(f\"Bronze Schema:     {CATALOG}.{BRONZE_SCHEMA}\")\n",
    "print(f\"Silver Schema:     {CATALOG}.{SILVER_SCHEMA}\")\n",
    "print(f\"Gold Schema:       {CATALOG}.{GOLD_SCHEMA}\")\n",
    "print(f\"Source Data:       {DATASET_BASE_PATH}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"You can now use these variables in your notebooks:\")\n",
    "print(\"  - CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA\")\n",
    "print(\"  - DATASET_BASE_PATH (for reading source files)\")\n",
    "print()\n",
    "print(\"Example:\")\n",
    "print(f\"  spark.read.csv('{DATASET_BASE_PATH}/customers/customers.csv')\")\n",
    "print(f\"  spark.sql('SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Access\n",
    "\n",
    "Check if you can access the shared training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: Verify Data Access\n",
    "# =============================================================================\n",
    "try:\n",
    "    files = dbutils.fs.ls(DATASET_BASE_PATH)\n",
    "    print(f\"Available data directories in {DATASET_BASE_PATH}:\")\n",
    "    for f in files:\n",
    "        print(f\"  - {f.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Cannot access {DATASET_BASE_PATH}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print()\n",
    "    print(\"Possible solutions:\")\n",
    "    print(\"  1. Ask trainer to create shared Volume and grant READ access\")\n",
    "    print(\"  2. Upload data manually to your catalog's Volume\")\n",
    "    print(f\"  3. Create Volume: CREATE VOLUME {CATALOG}.default.datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Trainer Setup (Run Once Before Training)\n",
    "\n",
    "**If you are the trainer**, run the cell below to:\n",
    "1. Create a group for training participants\n",
    "2. Create shared data Volume\n",
    "3. Grant permissions to all participants\n",
    "\n",
    "This only needs to be run once before the training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINER SETUP - Run once before training\n",
    "# =============================================================================\n",
    "# Uncomment and run if you are the trainer\n",
    "\n",
    "# TRAINING_GROUP = \"training_participants\"\n",
    "# SHARED_CATALOG = \"training_shared\"\n",
    "\n",
    "# # Step 1: Create shared catalog for data\n",
    "# spark.sql(f\"CREATE CATALOG IF NOT EXISTS {SHARED_CATALOG}\")\n",
    "# spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SHARED_CATALOG}.datasets\")\n",
    "\n",
    "# # Step 2: Create Volume for shared data files\n",
    "# spark.sql(f\"\"\"\n",
    "#     CREATE VOLUME IF NOT EXISTS {SHARED_CATALOG}.datasets.files\n",
    "#     COMMENT 'Shared training data files (read-only for participants)'\n",
    "# \"\"\")\n",
    "\n",
    "# # Step 3: Grant permissions to training group\n",
    "# # First create group in Account Console or via SCIM\n",
    "# spark.sql(f\"GRANT USE CATALOG ON CATALOG {SHARED_CATALOG} TO `{TRAINING_GROUP}`\")\n",
    "# spark.sql(f\"GRANT USE SCHEMA ON SCHEMA {SHARED_CATALOG}.datasets TO `{TRAINING_GROUP}`\")\n",
    "# spark.sql(f\"GRANT READ VOLUME ON VOLUME {SHARED_CATALOG}.datasets.files TO `{TRAINING_GROUP}`\")\n",
    "\n",
    "# # Step 4: Grant CREATE CATALOG to training group (for per-user isolation)\n",
    "# spark.sql(f\"GRANT CREATE CATALOG ON METASTORE TO `{TRAINING_GROUP}`\")\n",
    "\n",
    "# # Step 5: List group members (for verification)\n",
    "# # Note: This requires Account Admin or using Databricks REST API\n",
    "# print(f\"Setup complete. Add users to group '{TRAINING_GROUP}' in Account Console.\")\n",
    "# print(f\"Upload training data to: /Volumes/{SHARED_CATALOG}/datasets/files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup (After Training)\n",
    "\n",
    "Run this to remove your training environment after the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP - Run after training to remove your catalog\n",
    "# =============================================================================\n",
    "# WARNING: This will DELETE all tables and data in your training catalog!\n",
    "\n",
    "# Uncomment to run:\n",
    "# spark.sql(f\"DROP CATALOG IF EXISTS {CATALOG} CASCADE\")\n",
    "# print(f\"Catalog '{CATALOG}' has been removed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0aaf6c",
   "metadata": {},
   "source": [
    "# Workshop 2: Ingestion Pipeline - COPY INTO & Auto Loader\n",
    "\n",
    "**Workshop Objectives:**\n",
    "- Implement batch ingestion using COPY INTO\n",
    "- Configure Auto Loader for streaming ingestion\n",
    "- Handle various file formats (CSV, JSON, Parquet)\n",
    "- Monitor and manage ingestion pipelines\n",
    "\n",
    "**Duration:** 90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understanding data ingestion methods in Databricks Lakehouse\n",
    "\n",
    "### COPY INTO - Batch Ingestion\n",
    "- **Purpose**: Load data from external files into Delta tables\n",
    "- **Idempotency**: Automatically tracks processed files, preventing duplicates\n",
    "- **Use case**: Scheduled batch jobs, one-time data migrations\n",
    "- **Supported formats**: CSV, JSON, Parquet, Avro, ORC, TEXT\n",
    "\n",
    "### Auto Loader - Streaming Ingestion\n",
    "- **Purpose**: Incrementally process new files as they arrive\n",
    "- **cloudFiles format**: Uses `.format(\"cloudFiles\")` for streaming read\n",
    "- **Schema inference**: Automatically detects and evolves schema\n",
    "- **Use case**: Near real-time processing, continuous data pipelines\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader |\n",
    "|---------|-----------|-------------|\n",
    "| Processing | Batch | Streaming |\n",
    "| File tracking | Built-in | Checkpoint-based |\n",
    "| Schema evolution | Manual | Automatic |\n",
    "| Scalability | Medium | High |\n",
    "| Cost | Per execution | Per file |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420c2ef",
   "metadata": {},
   "source": [
    "## ðŸ“š Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9347b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0635808",
   "metadata": {},
   "source": [
    "## Part 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "### Task 1.1: CSV File Ingestion\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare target table `bronze_customers_batch`\n",
    "2. Use `COPY INTO` to load data from `customers.csv`\n",
    "3. Verify the number of loaded records\n",
    "\n",
    "**Hints:**\n",
    "- Use `BRONZE_PATH` variable for table location\n",
    "- Use `SOURCE_DATA_PATH` for source file path\n",
    "- Format options: `header`, `inferSchema` should be `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create target table\n",
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch (\n",
    " customer_id INT,\n",
    " name STRING,\n",
    " email STRING,\n",
    " city STRING,\n",
    " country STRING,\n",
    " _ingestion_timestamp TIMESTAMP\n",
    " )\n",
    " USING DELTA\n",
    " LOCATION '{____}/customers_batch' -- Complete with BRONZE_PATH\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b63d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO from CSV file\n",
    "spark.sql(f\"\"\"\n",
    " ____ INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    " FROM (\n",
    " SELECT \n",
    " customer_id,\n",
    " name,\n",
    " email,\n",
    " city,\n",
    " country,\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{____}/____' -- Complete with SOURCE_DATA_PATH and filename\n",
    " )\n",
    " FILEFORMAT = ____ -- Complete format (CSV)\n",
    " FORMAT_OPTIONS (\n",
    " 'header' = '____', -- Does file have header?\n",
    " 'inferSchema' = '____' -- Infer schema?\n",
    " )\n",
    " COPY_OPTIONS (\n",
    " 'mergeSchema' = '____' -- Merge schema?\n",
    " )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as total_records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cffcff",
   "metadata": {},
   "source": [
    "### Task 1.2: JSON File Ingestion\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare table `bronze_orders_batch`\n",
    "2. Use `COPY INTO` to load data from `orders_batch.json`\n",
    "3. Handle nested JSON structure\n",
    "\n",
    "**Hints:**\n",
    "- Use `DELTA` as table format\n",
    "- Use `BRONZE_PATH` for location\n",
    "- FILEFORMAT should be `JSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e72509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create table for orders\n",
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch (\n",
    " order_id INT,\n",
    " customer_id INT,\n",
    " order_date DATE,\n",
    " total_amount DOUBLE,\n",
    " status STRING,\n",
    " _ingestion_timestamp TIMESTAMP\n",
    " )\n",
    " USING ____ -- Complete format (DELTA)\n",
    " LOCATION '{____}/orders_batch' -- Complete path\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO from JSON file\n",
    "spark.sql(f\"\"\"\n",
    " COPY INTO {CATALOG}.{SCHEMA}.bronze_orders_batch\n",
    " FROM (\n",
    " SELECT \n",
    " order_id,\n",
    " customer_id,\n",
    " TO_DATE(order_date) as order_date,\n",
    " total_amount,\n",
    " status,\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{____}/____' -- Complete path to JSON\n",
    " )\n",
    " FILEFORMAT = ____ -- Complete format\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "spark.sql(f\"\"\"\n",
    " SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_batch LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44cfc5",
   "metadata": {},
   "source": [
    "### Task 1.3: Parquet File Ingestion\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare table `bronze_products_batch`\n",
    "2. Use `COPY INTO` to load data from `products.parquet`\n",
    "3. Add column with source file metadata\n",
    "\n",
    "**Hints:**\n",
    "- Use `_metadata.file_path` to get source file path\n",
    "- FILEFORMAT should be `PARQUET`\n",
    "- Source file: `products.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5561ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create table for products\n",
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch (\n",
    " product_id INT,\n",
    " product_name STRING,\n",
    " category STRING,\n",
    " price DOUBLE,\n",
    " stock_quantity INT,\n",
    " _source_file STRING,\n",
    " _ingestion_timestamp TIMESTAMP\n",
    " )\n",
    " USING DELTA\n",
    " LOCATION '{____}/products_batch'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316176df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO from Parquet file\n",
    "spark.sql(f\"\"\"\n",
    " COPY INTO {CATALOG}.{SCHEMA}.bronze_products_batch\n",
    " FROM (\n",
    " SELECT \n",
    " product_id,\n",
    " product_name,\n",
    " category,\n",
    " price,\n",
    " stock_quantity,\n",
    " ____ as _source_file, -- Use _metadata.file_path\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{____}/____' -- Complete path to Parquet\n",
    " )\n",
    " FILEFORMAT = ____\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "spark.sql(f\"\"\"\n",
    " SELECT product_id, product_name, category, _source_file \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_products_batch \n",
    " LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf241e",
   "metadata": {},
   "source": [
    "### Task 1.4: Idempotency - Re-running COPY INTO\n",
    "\n",
    "**Instructions:**\n",
    "1. Re-run `COPY INTO` for the same table\n",
    "2. Verify that data was not duplicated\n",
    "3. Check `COPY INTO` operation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check record count before re-running COPY INTO\n",
    "before_count = spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as count \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Record count before re-running COPY INTO: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-execute COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    " COPY INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    " FROM (\n",
    " SELECT \n",
    " customer_id,\n",
    " name,\n",
    " email,\n",
    " city,\n",
    " country,\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{SOURCE_DATA_PATH}/customers.csv'\n",
    " )\n",
    " FILEFORMAT = CSV\n",
    " FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idempotency verification\n",
    "after_count = spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as count \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Record count after re-running COPY INTO: {after_count}\")\n",
    "print(f\"Was data duplicated? {before_count != after_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9637c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Auto Loader - Streaming Ingestion\n",
    "\n",
    "### Task 2.1: Configuring Auto Loader for CSV\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare checkpoint location\n",
    "2. Use `.format(\"cloudFiles\")` to create streaming read\n",
    "3. Configure schema inference and evolution\n",
    "4. Write stream to `bronze_customers_stream` table\n",
    "\n",
    "**Hints:**\n",
    "- Format: `cloudFiles`\n",
    "- cloudFiles.format: `csv`\n",
    "- Schema location: use `CHECKPOINT_PATH` + subfolder name\n",
    "- Use `current_timestamp()` and `input_file_name()` for metadata\n",
    "- Output mode: `append`, mergeSchema: `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Streaming read with Auto Loader\n",
    "customers_stream = (\n",
    " spark.readStream\n",
    " .format(\"____\") # Complete format (cloudFiles)\n",
    " .option(\"cloudFiles.format\", \"____\") # Source file format (csv)\n",
    " .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/____\") # Schema checkpoint\n",
    " .option(\"header\", \"true\")\n",
    " .load(f\"{SOURCE_DATA_PATH}/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add metadata columns\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "customers_enriched = (\n",
    " customers_stream\n",
    " .withColumn(\"_ingestion_timestamp\", ____) # Add timestamp\n",
    " .withColumn(\"_source_file\", ____) # Add source file name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write stream to Delta table\n",
    "query_customers = (\n",
    " customers_enriched.writeStream\n",
    " .format(\"____\") # Complete format\n",
    " .outputMode(\"____\") # Write mode (append)\n",
    " .option(\"checkpointLocation\", f\"{____}/customers_stream\") # Checkpoint\n",
    " .option(\"mergeSchema\", \"____\") # Schema evolution\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream verification\n",
    "import time\n",
    "time.sleep(10) # Wait for processing\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as total_records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130ce52",
   "metadata": {},
   "source": [
    "### Task 2.2: Auto Loader for JSON with Schema Hints\n",
    "\n",
    "**Instructions:**\n",
    "1. Use Auto Loader to read `orders_batch.json`\n",
    "2. Add schema hints for columns with specific types\n",
    "3. Configure rescue data column for invalid records\n",
    "\n",
    "**Hints:**\n",
    "- cloudFiles.format: `json`\n",
    "- schemaHints example: `\"order_date DATE, total_amount DOUBLE\"`\n",
    "- rescuedDataColumn: `_rescued_data`\n",
    "- Checkpoint subfolder: `orders_stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc65dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto Loader with schema hints\n",
    "orders_stream = (\n",
    " spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"____\") # JSON format\n",
    " .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    " .option(\"cloudFiles.schemaHints\", \"____\") # Hint: \"order_date DATE, total_amount DOUBLE\"\n",
    " .option(\"cloudFiles.rescuedDataColumn\", \"____\") # Column for rescue data\n",
    " .load(f\"{____}/orders_batch.json\") # Complete path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write stream\n",
    "query_orders = (\n",
    " orders_stream.writeStream\n",
    " .format(\"delta\")\n",
    " .outputMode(\"____\") # Append mode\n",
    " .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\") # Checkpoint\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "time.sleep(10)\n",
    "spark.sql(f\"\"\"\n",
    " SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_stream LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645db40",
   "metadata": {},
   "source": [
    "### Task 2.3: Monitoring Streaming Queries\n",
    "\n",
    "**Instructions:**\n",
    "1. Display active streaming queries\n",
    "2. Check status and last progress of each query\n",
    "3. Retrieve metrics: number of processed records, batch duration\n",
    "\n",
    "**Hints:**\n",
    "- Use `spark.streams.active` to get active streams\n",
    "- Use `stream.lastProgress` to get last progress metrics\n",
    "- Progress contains: `batchId`, `numInputRows`, `batchDuration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display active streams\n",
    "active_streams = spark.streams.____ # Complete method (active)\n",
    "\n",
    "print(f\"Number of active streams: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    " print(f\"\\nStream ID: {stream.id}\")\n",
    " print(f\"Name: {stream.name}\")\n",
    " print(f\"Status: {stream.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6974ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check last progress\n",
    "if len(active_streams) > 0:\n",
    " last_progress = active_streams[0].____ # Complete method (lastProgress)\n",
    " \n",
    " if last_progress:\n",
    " print(f\"Batch ID: {last_progress['batchId']}\")\n",
    " print(f\"Processed records: {last_progress['numInputRows']}\")\n",
    " print(f\"Processing time: {last_progress['batchDuration']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55841f5d",
   "metadata": {},
   "source": [
    "### Task 2.4: Stopping Streaming Queries\n",
    "\n",
    "**Instructions:**\n",
    "1. Stop all active streaming queries\n",
    "2. Verify that all streams are stopped\n",
    "\n",
    "**Hints:**\n",
    "- Use `stream.stop()` method to stop a stream\n",
    "- Iterate over `spark.streams.active` to stop all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae308ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Stop all streams\n",
    "for stream in spark.streams.active:\n",
    " print(f\"Stopping stream: {stream.name}\")\n",
    " stream.____() # Complete method (stop)\n",
    "\n",
    "print(\"\\nAll streams stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(f\"Number of active streams: {len(spark.streams.active)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9819090",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Part 3: COPY INTO vs Auto Loader Comparison\n",
    "\n",
    "### Task 3.1: Performance Analysis\n",
    "\n",
    "**Instructions:**\n",
    "1. Compare record counts loaded by COPY INTO vs Auto Loader\n",
    "2. Check operation history for both methods\n",
    "3. Identify use cases for each method\n",
    "\n",
    "**Hints:**\n",
    "- Use `DESCRIBE HISTORY` to check table history\n",
    "- Compare `version`, `operation`, `operationMetrics` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbad7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record count comparison\n",
    "copy_into_count = spark.sql(f\"\"\"\n",
    " SELECT 'COPY INTO' as method, COUNT(*) as records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\")\n",
    "\n",
    "auto_loader_count = spark.sql(f\"\"\"\n",
    " SELECT 'Auto Loader' as method, COUNT(*) as records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\")\n",
    "\n",
    "copy_into_count.union(auto_loader_count).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO operation history\n",
    "spark.sql(f\"\"\"\n",
    " ____ HISTORY {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto Loader operation history\n",
    "spark.sql(f\"\"\"\n",
    " DESCRIBE HISTORY {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956008d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workshop Summary\n",
    "\n",
    "**Achieved Objectives:**\n",
    "- Batch ingestion implementation with COPY INTO\n",
    "- Auto Loader configuration for streaming ingestion\n",
    "- Handling different formats (CSV, JSON, Parquet)\n",
    "- Pipeline monitoring and management\n",
    "\n",
    "**When to Use COPY INTO:**\n",
    "- Batch processing with defined schedule\n",
    "- Known and stable data structure\n",
    "- Need for control over loading process\n",
    "- Out-of-the-box idempotency\n",
    "\n",
    "**When to Use Auto Loader:**\n",
    "- Near real-time processing\n",
    "- Schema evolution and automatic inference\n",
    "- Continuous monitoring for new files\n",
    "- Scalability and cost efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0913a18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Solutions\n",
    "\n",
    "Below are the complete solutions for all workshop tasks. Use them to verify your work or if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOLUTIONS - Part 1: COPY INTO - Batch Ingestion\n",
    "# =============================================================================\n",
    "\n",
    "# Task 1.1: CSV File Ingestion\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create target table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch (\n",
    "        customer_id INT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        city STRING,\n",
    "        country STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION '{BRONZE_PATH}/customers_batch'\n",
    "\"\"\")\n",
    "\n",
    "# COPY INTO from CSV file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            name,\n",
    "            email,\n",
    "            city,\n",
    "            country,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{SOURCE_DATA_PATH}/customers.csv'\n",
    "    )\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS (\n",
    "        'header' = 'true',\n",
    "        'inferSchema' = 'true'\n",
    "    )\n",
    "    COPY_OPTIONS (\n",
    "        'mergeSchema' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Task 1.2: JSON File Ingestion\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create table for orders\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch (\n",
    "        order_id INT,\n",
    "        customer_id INT,\n",
    "        order_date DATE,\n",
    "        total_amount DOUBLE,\n",
    "        status STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION '{BRONZE_PATH}/orders_batch'\n",
    "\"\"\")\n",
    "\n",
    "# COPY INTO from JSON file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_orders_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            TO_DATE(order_date) as order_date,\n",
    "            total_amount,\n",
    "            status,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{SOURCE_DATA_PATH}/orders_batch.json'\n",
    "    )\n",
    "    FILEFORMAT = JSON\n",
    "\"\"\")\n",
    "\n",
    "# Task 1.3: Parquet File Ingestion\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create table for products\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch (\n",
    "        product_id INT,\n",
    "        product_name STRING,\n",
    "        category STRING,\n",
    "        price DOUBLE,\n",
    "        stock_quantity INT,\n",
    "        _source_file STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION '{BRONZE_PATH}/products_batch'\n",
    "\"\"\")\n",
    "\n",
    "# COPY INTO from Parquet file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_products_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            product_name,\n",
    "            category,\n",
    "            price,\n",
    "            stock_quantity,\n",
    "            _metadata.file_path as _source_file,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{SOURCE_DATA_PATH}/products.parquet'\n",
    "    )\n",
    "    FILEFORMAT = PARQUET\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Part 2: Auto Loader - Streaming Ingestion\n",
    "# =============================================================================\n",
    "\n",
    "# Task 2.1: Configuring Auto Loader for CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# Streaming read with Auto Loader\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/customers_schema\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(f\"{SOURCE_DATA_PATH}/customers.csv\")\n",
    ")\n",
    "\n",
    "# Add metadata columns\n",
    "customers_enriched = (\n",
    "    customers_stream\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"_source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Write stream to Delta table\n",
    "query_customers = (\n",
    "    customers_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/customers_stream\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    ")\n",
    "\n",
    "# Task 2.2: Auto Loader for JSON with Schema Hints\n",
    "# -----------------------------------------------------------------------------\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    "    .option(\"cloudFiles.schemaHints\", \"order_date DATE, total_amount DOUBLE\")\n",
    "    .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "    .load(f\"{SOURCE_DATA_PATH}/orders_batch.json\")\n",
    ")\n",
    "\n",
    "# Write stream\n",
    "query_orders = (\n",
    "    orders_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/orders_stream\")\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    ")\n",
    "\n",
    "# Task 2.3: Monitoring Streaming Queries\n",
    "# -----------------------------------------------------------------------------\n",
    "# Display active streams\n",
    "active_streams = spark.streams.active\n",
    "\n",
    "print(f\"Number of active streams: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    "    print(f\"\\nStream ID: {stream.id}\")\n",
    "    print(f\"Name: {stream.name}\")\n",
    "    print(f\"Status: {stream.status}\")\n",
    "\n",
    "# Check last progress\n",
    "if len(active_streams) > 0:\n",
    "    last_progress = active_streams[0].lastProgress\n",
    "    \n",
    "    if last_progress:\n",
    "        print(f\"Batch ID: {last_progress['batchId']}\")\n",
    "        print(f\"Processed records: {last_progress['numInputRows']}\")\n",
    "        print(f\"Processing time: {last_progress['batchDuration']} ms\")\n",
    "\n",
    "# Task 2.4: Stopping Streaming Queries\n",
    "# -----------------------------------------------------------------------------\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping stream: {stream.name}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"\\nAll streams stopped!\")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Part 3: COPY INTO vs Auto Loader Comparison\n",
    "# =============================================================================\n",
    "\n",
    "# Task 3.1: COPY INTO operation history\n",
    "spark.sql(f\"\"\"\n",
    "    DESCRIBE HISTORY {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0cb7c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§¹ Resource Cleanup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Run only if you want to delete all created tables\n",
    "\n",
    "# Uncomment the lines below to delete tables:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    "\n",
    "# Clean up checkpoints\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, recurse=True)\n",
    "\n",
    "print(\"Resource cleanup is commented out. Uncomment to delete tables.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

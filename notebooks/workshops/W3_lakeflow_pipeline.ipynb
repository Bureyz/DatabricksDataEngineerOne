{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d50c8a2",
   "metadata": {},
   "source": [
    "# Workshop 3: Lakeflow Pipelines (Spark Declarative Pipelines)\n",
    "\n",
    "## The Story\n",
    "\n",
    "We are building a Modern Data Warehouse for a retail company. We have raw CSV data landing in our Data Lake, and we need to transform it into a high-quality **Star Schema** for reporting.\n",
    "\n",
    "We will use **Databricks Lakeflow Pipelines** to build a declarative pipeline that handles:\n",
    "1.  **Ingestion (Bronze):** Automatically loading new files using Auto Loader.\n",
    "2.  **Transformation (Silver):** Cleaning data, joining tables, and handling Slowly Changing Dimensions (SCD).\n",
    "3.  **Aggregation (Gold):** Creating business-ready tables for BI.\n",
    "\n",
    "## The Data Model (Star Schema)\n",
    "\n",
    "We will build the following schema from our source CSV files:\n",
    "\n",
    "### 1. Fact Table: `fact_sales`\n",
    "*   **Source:** Joins `SalesOrderHeader` (Orders) and `SalesOrderDetail` (Line Items).\n",
    "*   **Grain:** One row per product in an order.\n",
    "*   **Business Logic:**\n",
    "    *   Filter only **Shipped** orders (`Status = 5`).\n",
    "    *   Calculate `TotalLineAmount` (if needed, or use `LineTotal`).\n",
    "*   **Quality Check (Expectations):**\n",
    "    *   `valid_qty`: `OrderQty > 0`\n",
    "    *   `valid_price`: `UnitPrice >= 0`\n",
    "\n",
    "### 2. Dimension: `dim_customer` (SCD Type 2)\n",
    "*   **Source:** `Customers.csv`\n",
    "*   **Behavior:** **SCD Type 2 (History)**. We want to track changes in customer details (e.g., Name, Email) over time.\n",
    "*   **Key Columns:** `CustomerID`, `FirstName`, `LastName`, `EmailAddress`, `CompanyName`.\n",
    "\n",
    "### 3. Dimension: `dim_product` (SCD Type 1)\n",
    "*   **Source:** Joins `Product.csv` and `ProductCategory.csv`.\n",
    "*   **Behavior:** **SCD Type 1 (Overwrite)**. If a product name or category changes, we just update the record. We don't need history for product typos.\n",
    "*   **Key Columns:** `ProductID`, `Name`, `ProductNumber`, `Color`, `CategoryName`.\n",
    "\n",
    "---\n",
    "\n",
    "## How to run this?\n",
    "\n",
    "This notebook is a **Lakeflow Pipeline definition**. You cannot run it cell-by-cell like a normal notebook!\n",
    "\n",
    "**Steps to create the Pipeline:**\n",
    "\n",
    "1.  **Go to Workflows -> Lakeflow Pipelines (Delta Live Tables)**.\n",
    "2.  **Click \"Create Pipeline\"**.\n",
    "3.  **Fill in the details:**\n",
    "    *   **Pipeline Name:** `workshop_lakeflow_pipeline`\n",
    "    *   **Product edition:** **Advanced** (Required for SCD Type 2).\n",
    "    *   **Pipeline mode:** **Triggered** (Best for workshops/batch) or Continuous.\n",
    "    *   **Source code:** Select THIS notebook (`W3_lakeflow_pipeline.ipynb`).\n",
    "    *   **Destination:** Unity Catalog.\n",
    "        *   **Catalog:** `workshop_catalog` (or your assigned catalog).\n",
    "        *   **Target Schema:** `workshop_lakeflow`.\n",
    "    *   **Configuration:**\n",
    "        *   Add Key: `source_path`\n",
    "        *   Value: `/Volumes/workspace/default/dataset/workshop/main/` (Check your actual path!)\n",
    "\n",
    "> **[PLACEHOLDER: Screenshot of Pipeline Configuration]**\n",
    "\n",
    "4.  **Click \"Create\"**.\n",
    "5.  **Click \"Start\"** to run the pipeline for the first time.\n",
    "\n",
    "> **[PLACEHOLDER: Screenshot of Running Pipeline Graph]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a77da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql.functions import col, current_timestamp, expr\n",
    "\n",
    "# We read the path from the Pipeline Configuration\n",
    "# Default to a dummy path if not set (for syntax checking in editor)\n",
    "source_path = spark.conf.get(\"source_path\", \"/Volumes/workspace/default/dataset/workshop/main/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cd2df",
   "metadata": {},
   "source": [
    "## Step 1: Bronze Layer (Ingestion)\n",
    "\n",
    "We use **Auto Loader** (`cloudFiles`) to incrementally ingest data from CSV files.\n",
    "We need to define 5 bronze tables:\n",
    "1.  `bronze_customers`\n",
    "2.  `bronze_products`\n",
    "3.  `bronze_categories`\n",
    "4.  `bronze_headers`\n",
    "5.  `bronze_details`\n",
    "\n",
    "**Hint:**\n",
    "*   Use the `@dp.table` decorator.\n",
    "*   Inside the function, use `spark.readStream.format(\"cloudFiles\")`.\n",
    "*   Set `cloudFiles.format` to `csv`.\n",
    "*   Use `inferSchema` or provide schema (for workshop `inferSchema` is fine).\n",
    "*   Load from `{source_path}/FileName.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b25267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define Bronze Tables\n",
    "\n",
    "# 1. bronze_customers\n",
    "@dp.table(\n",
    "    comment=\"Raw customers data ingested from CSV\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/Customers.csv\")\n",
    "    )\n",
    "\n",
    "# TODO: Implement the rest of bronze tables (products, categories, headers, details)\n",
    "# ...\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d82278",
   "metadata": {},
   "source": [
    "## Step 2: Silver Layer (Dimensions & SCD)\n",
    "\n",
    "### 2.1 Dimension: Customers (SCD Type 2)\n",
    "\n",
    "We want to track history of customer changes.\n",
    "*   **Target Table:** `dim_customer`\n",
    "*   **Source:** `bronze_customers`\n",
    "*   **Keys:** `CustomerID`\n",
    "*   **Sequence By:** `ModifiedDate` (to determine order of updates)\n",
    "*   **SCD Type:** 2\n",
    "\n",
    "**Hint:**\n",
    "Use `dp.create_streaming_table(\"table_name\")` first.\n",
    "Then use `dp.create_auto_cdc_flow(...)`.\n",
    "\n",
    "### 2.2 Dimension: Products (SCD Type 1)\n",
    "\n",
    "We want to overwrite changes (no history).\n",
    "First, we need to **JOIN** `bronze_products` with `bronze_categories` to get the Category Name.\n",
    "\n",
    "**Hint:**\n",
    "1.  Create a helper table `@dp.table` named `products_joined` that performs the join.\n",
    "2.  Create the target streaming table `dim_product`.\n",
    "3.  Use `dp.create_auto_cdc_flow(...)` with `stored_as_scd_type = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define dim_customer (SCD Type 2)\n",
    "# dp.create_streaming_table(\"dim_customer\")\n",
    "# dp.create_auto_cdc_flow(...)\n",
    "\n",
    "\n",
    "# TODO: Define products_joined (Helper Table)\n",
    "# @dp.table\n",
    "# def products_joined():\n",
    "#     ...\n",
    "\n",
    "\n",
    "# TODO: Define dim_product (SCD Type 1)\n",
    "# dp.create_streaming_table(\"dim_product\")\n",
    "# dp.create_auto_cdc_flow(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93598b98",
   "metadata": {},
   "source": [
    "## Step 3: Gold Layer (Fact Table)\n",
    "\n",
    "### 3.1 Fact: Sales\n",
    "\n",
    "We need to create `fact_sales` by joining `bronze_headers` and `bronze_details`.\n",
    "We also want to enforce **Data Quality** expectations.\n",
    "\n",
    "**Requirements:**\n",
    "1.  Join `bronze_headers` (h) and `bronze_details` (d) on `SalesOrderID`.\n",
    "2.  Filter for `Status = 5` (Shipped).\n",
    "3.  Select relevant columns: `SalesOrderID`, `OrderDate`, `CustomerID`, `ProductID`, `OrderQty`, `UnitPrice`, `LineTotal`.\n",
    "4.  **Expectation:** Drop rows where `OrderQty <= 0`.\n",
    "\n",
    "**Hint:**\n",
    "Use `@dp.expect_or_drop(\"name\", \"condition\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define fact_sales\n",
    "# @dp.table\n",
    "# @dp.expect_or_drop(\"valid_qty\", \"OrderQty > 0\")\n",
    "# def fact_sales():\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329c709",
   "metadata": {},
   "source": [
    "## Step 4: Orchestration (Theory)\n",
    "\n",
    "Once the pipeline is created, it can be scheduled as a **Job**.\n",
    "\n",
    "1.  Go to **Workflows -> Jobs**.\n",
    "2.  Create a new Job.\n",
    "3.  Task Type: **Pipeline**.\n",
    "4.  Select your `workshop_lakeflow_pipeline`.\n",
    "5.  Set Schedule (e.g., \"Every day at 8:00 AM\").\n",
    "\n",
    "This allows you to treat your ETL pipeline just like any other scheduled task in a Data Warehouse.\n",
    "\n",
    "---\n",
    "\n",
    "# Solution\n",
    "\n",
    "Below is the complete code for the pipeline. You can copy-paste this if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d608a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 3: Lakeflow Pipelines\n",
    "# ============================================================\n",
    "\n",
    "# --- BRONZE LAYER ---\n",
    "\n",
    "@dp.table(comment=\"Raw customers data\")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/Customers.csv\")\n",
    "    )\n",
    "\n",
    "@dp.table(comment=\"Raw products data\")\n",
    "def bronze_products():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/Product.csv\")\n",
    "    )\n",
    "\n",
    "@dp.table(comment=\"Raw product categories data\")\n",
    "def bronze_categories():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/ProductCategory.csv\")\n",
    "    )\n",
    "\n",
    "@dp.table(comment=\"Raw sales headers\")\n",
    "def bronze_headers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/SalesOrderHeader.csv\")\n",
    "    )\n",
    "\n",
    "@dp.table(comment=\"Raw sales details\")\n",
    "def bronze_details():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/SalesOrderDetail.csv\")\n",
    "    )\n",
    "\n",
    "# --- SILVER LAYER (DIMENSIONS) ---\n",
    "\n",
    "# SCD Type 2 for Customers\n",
    "dp.create_streaming_table(\"dim_customer\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target = \"dim_customer\",\n",
    "    source = \"bronze_customers\",\n",
    "    keys = [\"CustomerID\"],\n",
    "    sequence_by = col(\"ModifiedDate\"),\n",
    "    stored_as_scd_type = 2\n",
    ")\n",
    "\n",
    "# SCD Type 1 for Products (with Join)\n",
    "@dp.table\n",
    "def products_joined():\n",
    "    p = dp.read(\"bronze_products\")\n",
    "    c = dp.read(\"bronze_categories\")\n",
    "    return p.join(c, p.ProductCategoryID == c.ProductCategoryID, \"left\") \\\n",
    "            .select(\n",
    "                p.ProductID, \n",
    "                p.Name.alias(\"ProductName\"), \n",
    "                p.ProductNumber, \n",
    "                p.Color,\n",
    "                c.Name.alias(\"CategoryName\"), \n",
    "                p.ModifiedDate\n",
    "            )\n",
    "\n",
    "dp.create_streaming_table(\"dim_product\")\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target = \"dim_product\",\n",
    "    source = \"products_joined\",\n",
    "    keys = [\"ProductID\"],\n",
    "    sequence_by = col(\"ModifiedDate\"),\n",
    "    stored_as_scd_type = 1\n",
    ")\n",
    "\n",
    "# --- GOLD LAYER (FACTS) ---\n",
    "\n",
    "@dp.table(comment=\"Fact table for sales analysis\")\n",
    "@dp.expect_or_drop(\"valid_qty\", \"OrderQty > 0\")\n",
    "@dp.expect_or_drop(\"valid_price\", \"UnitPrice >= 0\")\n",
    "def fact_sales():\n",
    "    h = dp.read(\"bronze_headers\")\n",
    "    d = dp.read(\"bronze_details\")\n",
    "    \n",
    "    # Filter for Shipped orders (Status = 5)\n",
    "    return h.filter(col(\"Status\") == 5) \\\n",
    "            .join(d, h.SalesOrderID == d.SalesOrderID, \"inner\") \\\n",
    "            .select(\n",
    "                h.SalesOrderID,\n",
    "                h.OrderDate,\n",
    "                h.CustomerID,\n",
    "                d.ProductID,\n",
    "                d.OrderQty,\n",
    "                d.UnitPrice,\n",
    "                d.LineTotal\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

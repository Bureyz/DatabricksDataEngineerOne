{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf439070",
   "metadata": {},
   "source": [
    "# Workshop 3: Lakeflow Pipelines (Spark Declarative Pipelines)\n",
    "\n",
    "## The Story\n",
    "\n",
    "We are building a modern Data Warehouse using **Lakeflow Pipelines** (formerly Delta Live Tables).\n",
    "Lakeflow uses **Spark Declarative Pipelines (SDP)** to define data flows.\n",
    "\n",
    "The business requires a **Star Schema** to analyze sales, with specific requirements for handling data changes:\n",
    "\n",
    "1.  **Products (SCD Type 1)**: If a product name changes, simply update it. We only care about the current name.\n",
    "2.  **Customers (SCD Type 2)**: If a customer updates their profile, we need to keep a history of changes.\n",
    "\n",
    "**Your Mission:**\n",
    "1.  **Ingest**: Create a **Bronze** layer using Auto Loader.\n",
    "2.  **Clean**: Create a **Silver** layer with data quality expectations.\n",
    "3.  **Model**: Use `APPLY CHANGES INTO` (Auto CDC) to implement **SCD Type 1** and **SCD Type 2** logic.\n",
    "4.  **Deploy**: Create a Lakeflow Pipeline job.\n",
    "\n",
    "**Note:** We will use the new `databricks.pipelines` (aliased as `dp`) module instead of the deprecated `dlt`.\n",
    "\n",
    "**Time:** 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27db8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This notebook is a Lakeflow Pipeline definition. \n",
    "# It is NOT meant to be run cell-by-cell interactively (except this setup block).\n",
    "# You will deploy this as a Pipeline in the \"Workflows\" tab.\n",
    "\n",
    "%run ../00_setup\n",
    "\n",
    "# --- INDEPENDENT SETUP ---\n",
    "# Ensure source files exist for the pipeline to read\n",
    "source_path = f\"{volume_path}/main\"\n",
    "required_files = [\"Customers.csv\", \"Product.csv\", \"SalesOrderDetail.csv\"]\n",
    "\n",
    "print(f\"Checking source data in {source_path}...\")\n",
    "try:\n",
    "    files = [f.name for f in dbutils.fs.ls(source_path)]\n",
    "    missing = [f for f in required_files if f not in files]\n",
    "    if missing:\n",
    "        print(f\"⚠️ Missing files: {missing}. Please upload them to {source_path}.\")\n",
    "    else:\n",
    "        print(f\"✅ All required source files found in {source_path}.\")\n",
    "        print(f\"   - Customers.csv\")\n",
    "        print(f\"   - Product.csv\")\n",
    "        print(f\"   - SalesOrderDetail.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking files: {e}\")\n",
    "\n",
    "# We will use this path in the Pipeline code\n",
    "print(f\"\\nCopy this path for your Pipeline code: {source_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fbd77",
   "metadata": {},
   "source": [
    "## Step 1: Bronze Layer (Ingestion)\n",
    "\n",
    "We use **Auto Loader** (`cloudFiles`) to ingest data incrementally.\n",
    "We need to define three bronze tables:\n",
    "1.  `bronze_customers`\n",
    "2.  `bronze_products`\n",
    "3.  `bronze_sales` (from SalesOrderDetail)\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "import databricks.pipelines as dp\n",
    "\n",
    "@dp.table\n",
    "def bronze_customers():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .load(f\"{source_path}/Customers.csv\")\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a159caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import databricks.pipelines as dp\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Hardcode the path for the Pipeline (since variables from other cells don't pass to the runtime easily)\n",
    "# REPLACE THIS with the path printed in the setup cell above!\n",
    "input_path = \"/Volumes/main/default/workshop_files/main\" \n",
    "\n",
    "# TODO: Define bronze_customers\n",
    "# @dp.table\n",
    "# def bronze_customers():\n",
    "#     return ...\n",
    "\n",
    "# TODO: Define bronze_products\n",
    "\n",
    "# TODO: Define bronze_sales (SalesOrderDetail.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7878155",
   "metadata": {},
   "source": [
    "## Step 2: Silver Layer (Cleaning)\n",
    "\n",
    "Create \"clean\" versions of our tables.\n",
    "*   **Customers**: Must have a valid `EmailAddress`.\n",
    "*   **Products**: Must have a `Name`.\n",
    "\n",
    "Use `@dp.expect_or_drop` to enforce these rules.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "@dp.table\n",
    "@dp.expect_or_drop(\"valid_email\", \"EmailAddress IS NOT NULL\")\n",
    "def silver_customers():\n",
    "  return dp.read(\"bronze_customers\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e461b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define silver_customers (Expect EmailAddress IS NOT NULL)\n",
    "\n",
    "# TODO: Define silver_products (Expect Name IS NOT NULL)\n",
    "\n",
    "# TODO: Define silver_sales (Pass through from bronze_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1d95a",
   "metadata": {},
   "source": [
    "## Step 3: Gold Layer - SCD Handling (Auto CDC)\n",
    "\n",
    "This is where Lakeflow Pipelines shines. We use `dp.apply_changes` (Auto CDC) to handle history.\n",
    "\n",
    "### SCD Type 1: Products\n",
    "We only want the **latest** version of a product.\n",
    "*   `stored_as_scd_type=1`\n",
    "*   Keys: `ProductID`\n",
    "*   Sequence: `ModifiedDate` (to know which is newer)\n",
    "\n",
    "### SCD Type 2: Customers\n",
    "We want a **history** of changes.\n",
    "*   `stored_as_scd_type=2`\n",
    "*   Keys: `CustomerID`\n",
    "*   Sequence: `ModifiedDate`\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "dp.create_streaming_table(\"dim_product\")\n",
    "\n",
    "dp.apply_changes(\n",
    "  target = \"dim_product\",\n",
    "  source = \"silver_products\",\n",
    "  keys = [\"ProductID\"],\n",
    "  sequence_by = col(\"ModifiedDate\"),\n",
    "  stored_as_scd_type = 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create dim_product (SCD Type 1)\n",
    "# dp.create_streaming_table(\"dim_product\")\n",
    "# dp.apply_changes(...)\n",
    "\n",
    "# TODO: Create dim_customer (SCD Type 2)\n",
    "# dp.create_streaming_table(\"dim_customer\")\n",
    "# dp.apply_changes(..., stored_as_scd_type = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7efdb",
   "metadata": {},
   "source": [
    "## Step 4: Gold Fact Table\n",
    "\n",
    "Finally, create the `fact_sales` table.\n",
    "In a Star Schema, the Fact table links to Dimensions via keys.\n",
    "\n",
    "**Hint:**\n",
    "```python\n",
    "@dp.table\n",
    "def fact_sales():\n",
    "  return dp.read(\"silver_sales\").select(\"SalesOrderID\", \"CustomerID\", \"ProductID\", \"OrderQty\", \"UnitPrice\", \"LineTotal\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define fact_sales\n",
    "# @dp.table\n",
    "# def fact_sales():\n",
    "#     return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 3: Lakeflow Pipelines (SDP)\n",
    "# ============================================================\n",
    "\n",
    "import databricks.pipelines as dp\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Path to source data (Update this!)\n",
    "input_path = \"/Volumes/main/default/workshop_files/main\"\n",
    "\n",
    "# --- Bronze Layer ---\n",
    "@dp.table\n",
    "def bronze_customers():\n",
    "    return (spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "            .load(f\"{input_path}/Customers.csv\"))\n",
    "\n",
    "@dp.table\n",
    "def bronze_products():\n",
    "    return (spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "            .load(f\"{input_path}/Product.csv\"))\n",
    "\n",
    "@dp.table\n",
    "def bronze_sales():\n",
    "    return (spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n",
    "            .load(f\"{input_path}/SalesOrderDetail.csv\"))\n",
    "\n",
    "# --- Silver Layer ---\n",
    "@dp.table\n",
    "@dp.expect_or_drop(\"valid_email\", \"EmailAddress IS NOT NULL\")\n",
    "def silver_customers():\n",
    "    return dp.read(\"bronze_customers\")\n",
    "\n",
    "@dp.table\n",
    "@dp.expect_or_drop(\"valid_name\", \"Name IS NOT NULL\")\n",
    "def silver_products():\n",
    "    return dp.read(\"bronze_products\")\n",
    "\n",
    "@dp.table\n",
    "def silver_sales():\n",
    "    return dp.read(\"bronze_sales\")\n",
    "\n",
    "# --- Gold Layer (SCD) ---\n",
    "\n",
    "# SCD Type 1: Products (Overwrite)\n",
    "dp.create_streaming_table(\"dim_product\")\n",
    "\n",
    "dp.apply_changes(\n",
    "    target = \"dim_product\",\n",
    "    source = \"silver_products\",\n",
    "    keys = [\"ProductID\"],\n",
    "    sequence_by = col(\"ModifiedDate\"),\n",
    "    stored_as_scd_type = 1\n",
    ")\n",
    "\n",
    "# SCD Type 2: Customers (History)\n",
    "dp.create_streaming_table(\"dim_customer\")\n",
    "\n",
    "dp.apply_changes(\n",
    "    target = \"dim_customer\",\n",
    "    source = \"silver_customers\",\n",
    "    keys = [\"CustomerID\"],\n",
    "    sequence_by = col(\"ModifiedDate\"),\n",
    "    stored_as_scd_type = 2\n",
    ")\n",
    "\n",
    "# Fact Table\n",
    "@dp.table\n",
    "def fact_sales():\n",
    "    return dp.read(\"silver_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2b85b",
   "metadata": {},
   "source": [
    "## Step 5: Deploying the Lakeflow Pipeline\n",
    "\n",
    "To run this code, you must create a Pipeline resource.\n",
    "\n",
    "1.  **Navigate**: Click **Workflows** in the sidebar, then **Delta Live Tables** (soon to be renamed **Lakeflow Pipelines**).\n",
    "2.  **Create Pipeline**:\n",
    "    *   **Pipeline Name**: `Lakeflow_Workshop_YourName`\n",
    "    *   **Product Edition**: `Advanced` (Required for SCD Type 2 / `APPLY CHANGES`).\n",
    "    *   **Source Code**: Select **this notebook**.\n",
    "    *   **Destination**: Unity Catalog (`catalog.schema`).\n",
    "3.  **Start**: Click **Start** to run the pipeline.\n",
    "\n",
    "### Observe:\n",
    "*   The **Graph** visualization showing dependencies.\n",
    "*   **Data Quality** metrics (how many records dropped in Silver).\n",
    "*   **SCD Handling**: Check `dim_customer` - it will have `__START_AT` and `__END_AT` columns automatically added!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

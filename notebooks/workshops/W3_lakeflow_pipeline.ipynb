{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d50c8a2",
   "metadata": {},
   "source": [
    "# Workshop 3: Lakeflow Pipelines\n",
    "\n",
    "## The Story\n",
    "\n",
    "We are building a Modern Data Warehouse for a retail company. We have raw CSV data landing in our Data Lake, and we need to transform it into a high-quality **Star Schema** for reporting.\n",
    "\n",
    "We will use **Databricks Lakeflow Pipelines** to build a declarative pipeline that handles:\n",
    "1.  **Ingestion (Bronze):** Automatically loading new files.\n",
    "2.  **Transformation (Silver):** Cleaning data and handling Slowly Changing Dimensions (SCD).\n",
    "3.  **Aggregation (Gold):** Creating business-ready tables.\n",
    "\n",
    "## The Data Model (Star Schema)\n",
    "\n",
    "We will build the following schema:\n",
    "\n",
    "### 1. Fact Table: `fact_sales`\n",
    "*   **Source:** Joins `SalesOrderHeader` and `SalesOrderDetail`.\n",
    "*   **Grain:** One row per product in an order.\n",
    "*   **Key Metrics:** `OrderQty`, `UnitPrice`, `LineTotal`.\n",
    "*   **Quality Check:** `OrderQty` must be greater than 0.\n",
    "\n",
    "### 2. Dimension: `dim_customer` (SCD Type 2)\n",
    "*   **Source:** `Customers.csv`\n",
    "*   **Behavior:** **SCD Type 2 (History)**. We want to track changes in customer details over time.\n",
    "*   **Key Columns:** `CustomerID`, `FirstName`, `LastName`, `Email`.\n",
    "\n",
    "### 3. Dimension: `dim_product` (SCD Type 1)\n",
    "*   **Source:** Joins `Product.csv` and `ProductCategory.csv`.\n",
    "*   **Behavior:** **SCD Type 1 (Overwrite)**. If a product name changes, we just update it. We don't need history for typos.\n",
    "*   **Key Columns:** `ProductID`, `Name`, `CategoryName`.\n",
    "\n",
    "---\n",
    "\n",
    "## How to run this?\n",
    "\n",
    "This notebook is a **Lakeflow Pipeline definition**. You cannot run it cell-by-cell like a normal notebook!\n",
    "\n",
    "**Steps to create the Pipeline:**\n",
    "1.  Go to **Lakeflow Pipelines**.\n",
    "2.  Click **Create Pipeline**.\n",
    "3.  **Product edition:** Advanced (required for SCD).\n",
    "4.  **Pipeline mode:** Triggered (for workshop) or Continuous.\n",
    "5.  **Source code:** Select THIS notebook.\n",
    "6.  **Destination:** Unity Catalog (Schema: `workshop_lakeflow`).\n",
    "7.  **Configuration:**\n",
    "    *   `source_path`: `/Volumes/workspace/default/dataset/workshop/main/` (Adjust to your path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a77da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "# We read the path from the Pipeline Configuration\n",
    "# Default to a dummy path if not set (for syntax checking)\n",
    "source_path = spark.conf.get(\"source_path\", \"/Volumes/workspace/default/dataset/workshop/main/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cd2df",
   "metadata": {},
   "source": [
    "## Step 1: Bronze Layer (Ingestion)\n",
    "\n",
    "We use **Auto Loader** (`cloudFiles`) to incrementally ingest data.\n",
    "Define 5 bronze tables: `bronze_customers`, `bronze_products`, `bronze_categories`, `bronze_headers`, `bronze_details`.\n",
    "\n",
    "**Hint:**\n",
    "Use the `@dlt.table` decorator.\n",
    "Inside the function, use `spark.readStream.format(\"cloudFiles\")`.\n",
    "Remember to set `cloudFiles.format` to `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b25267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define Bronze Tables\n",
    "# 1. bronze_customers\n",
    "# 2. bronze_products\n",
    "# 3. bronze_categories\n",
    "# 4. bronze_headers\n",
    "# 5. bronze_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d82278",
   "metadata": {},
   "source": [
    "## Step 2: Silver Layer (Dimensions & SCD)\n",
    "\n",
    "### 2.1 Dimension: Customers (SCD Type 2)\n",
    "\n",
    "We want to track history. Use `dlt.apply_changes`.\n",
    "\n",
    "**Hint:**\n",
    "Use `dlt.create_streaming_table(\"table_name\")` first.\n",
    "Then use `dlt.apply_changes(...)`.\n",
    "Key parameters: `target`, `source`, `keys`, `sequence_by`, and `stored_as_scd_type = 2`.\n",
    "\n",
    "### 2.2 Dimension: Products (SCD Type 1)\n",
    "\n",
    "We want to overwrite changes. First, join Products with Categories.\n",
    "\n",
    "**Hint:**\n",
    "1. Create a helper table `@dlt.table` that joins products and categories.\n",
    "2. Create the target streaming table.\n",
    "3. Use `dlt.apply_changes(...)` with `stored_as_scd_type = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define dim_customer (SCD Type 2)\n",
    "\n",
    "\n",
    "# TODO: Define intermediate joined table for products\n",
    "\n",
    "\n",
    "# TODO: Define dim_product (SCD Type 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af60f0",
   "metadata": {},
   "source": [
    "## Step 3: Silver Layer (Fact Table)\n",
    "\n",
    "Create `fact_sales` by joining Headers and Details.\n",
    "Add a **Quality Expectation** to drop invalid rows.\n",
    "\n",
    "**Hint:**\n",
    "Use `@dlt.table` and `@dlt.expect_or_drop(...)`.\n",
    "Read source tables using `dlt.read(\"table_name\")`.\n",
    "Perform the join in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62da8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define fact_sales with Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56600433",
   "metadata": {},
   "source": [
    "## Step 4: Gold Layer (Aggregation)\n",
    "\n",
    "Create a business report: `sales_by_category`.\n",
    "Calculate total sales per category.\n",
    "\n",
    "**Hint:**\n",
    "Join `fact_sales` and `dim_product`.\n",
    "Group by category and sum the sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define sales_by_category report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af289a",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "The complete code is below.\n",
    "\n",
    "---\n",
    "**Scheduling Note:**\n",
    "To schedule this pipeline:\n",
    "1. Go to the Pipeline settings.\n",
    "2. Click **Schedule**.\n",
    "3. Choose \"Scheduled\" and set the cron syntax (e.g., every hour).\n",
    "This works exactly like a Job!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2708ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL SOLUTION - Workshop 3: Lakeflow Pipelines\n",
    "# ============================================================\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "source_path = spark.conf.get(\"source_path\", \"/Volumes/workspace/default/dataset/workshop/main/\")\n",
    "\n",
    "# --- BRONZE LAYER ---\n",
    "\n",
    "@dlt.table\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/Customers.csv\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def bronze_products():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/Product.csv\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def bronze_categories():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/ProductCategory.csv\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def bronze_headers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/SalesOrderHeader.csv\")\n",
    "    )\n",
    "\n",
    "@dlt.table\n",
    "def bronze_details():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(f\"{source_path}/SalesOrderDetail.csv\")\n",
    "    )\n",
    "\n",
    "# --- SILVER LAYER (DIMENSIONS) ---\n",
    "\n",
    "# SCD Type 2 for Customers\n",
    "dlt.create_streaming_table(\"dim_customer\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target = \"dim_customer\",\n",
    "    source = \"bronze_customers\",\n",
    "    keys = [\"CustomerID\"],\n",
    "    sequence_by = col(\"ModifiedDate\"),\n",
    "    stored_as_scd_type = 2\n",
    ")\n",
    "\n",
    "# SCD Type 1 for Products (with Join)\n",
    "@dlt.table\n",
    "def products_joined():\n",
    "    p = dlt.read(\"bronze_products\")\n",
    "    c = dlt.read(\"bronze_categories\")\n",
    "    return p.join(c, p.ProductCategoryID == c.ProductCategoryID, \"left\") \\\n",
    "            .select(p.ProductID, p.Name.alias(\"ProductName\"), p.ProductNumber, c.Name.alias(\"CategoryName\"), p.ModifiedDate)\n",
    "\n",
    "dlt.create_streaming_table(\"dim_product\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target = \"dim_product\",\n",
    "    source = \"products_joined\",\n",
    "    keys = [\"ProductID\"],\n",
    "    sequence_by = col(\"ModifiedDate\"),\n",
    "    stored_as_scd_type = 1\n",
    ")\n",
    "\n",
    "# --- SILVER LAYER (FACTS) ---\n",
    "\n",
    "@dlt.table\n",
    "@dlt.expect_or_drop(\"valid_qty\", \"OrderQty > 0\")\n",
    "def fact_sales():\n",
    "    h = dlt.read(\"bronze_headers\")\n",
    "    d = dlt.read(\"bronze_details\")\n",
    "    \n",
    "    return h.join(d, h.SalesOrderID == d.SalesOrderID, \"inner\") \\\n",
    "            .select(\n",
    "                h.SalesOrderID,\n",
    "                h.OrderDate,\n",
    "                h.CustomerID,\n",
    "                d.ProductID,\n",
    "                d.OrderQty,\n",
    "                d.UnitPrice,\n",
    "                d.LineTotal\n",
    "            )\n",
    "\n",
    "# --- GOLD LAYER ---\n",
    "\n",
    "@dlt.table\n",
    "def sales_by_category():\n",
    "    f = dlt.read(\"fact_sales\")\n",
    "    p = dlt.read(\"dim_product\")\n",
    "    \n",
    "    return f.join(p, f.ProductID == p.ProductID, \"inner\") \\\n",
    "            .groupBy(\"CategoryName\") \\\n",
    "            .sum(\"LineTotal\") \\\n",
    "            .withColumnRenamed(\"sum(LineTotal)\", \"TotalSales\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

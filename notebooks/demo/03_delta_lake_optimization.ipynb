{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17354d4a",
   "metadata": {},
   "source": [
    "# Delta Lake Operations\n",
    "\n",
    "## The Story: From \"Data Swamp\" to Reliable Data\n",
    "\n",
    "Your e-commerce company's previous data platform was a **Data Lake disaster**:\n",
    "\n",
    "- Marketing updated customer segments, breaking downstream reports\n",
    "- A failed job left half-written files - data corrupted for 2 days\n",
    "- Finance asked for \"data from last month\" - no way to get it\n",
    "- Data Scientists couldn't trust the data - \"is this the latest version?\"\n",
    "\n",
    "**Delta Lake solves these problems.** This is the most important technology in modern Lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9ff36",
   "metadata": {},
   "source": [
    "## Theoretical Introduction\n",
    "\n",
    "**Notebook Objective:** Comprehensive introduction to Delta Lake as a transactional storage layer over Data Lake\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Delta Lake**: Open-source storage layer providing ACID transactions for Apache Spark\n",
    "- **Delta Log**: Transactional log storing metadata about all table changes\n",
    "- **Schema Enforcement & Evolution**: Automatic schema validation and controlled evolution\n",
    "- **Time Travel**: Ability to access previous versions of data\n",
    "- **Optimization**: Techniques to improve query performance (OPTIMIZE, Z-ORDER, Liquid Clustering)\n",
    "\n",
    "**Why is this important?**\n",
    "Delta Lake solves fundamental Data Lake problems: lack of transactions, schema drift, update difficulties, and quality assurance. It provides Data Warehouse reliability with Data Lake flexibility.\n",
    "\n",
    "**Notebook Structure:**\n",
    "1. **Section 1**: Delta Lake Core Features (Creating Tables, Schema Enforcement, Schema Evolution, Constraints)\n",
    "2. **Section 2**: CRUD Operations & MERGE (INSERT, UPDATE, DELETE, MERGE INTO)\n",
    "3. **Section 3**: Metadata and Analytics (DESCRIBE DETAIL, DESCRIBE HISTORY, Delta Log Internals)\n",
    "4. **Section 4**: Time Travel & Disaster Recovery (VERSION AS OF, RESTORE, VACUUM implications)\n",
    "5. **Section 5**: Optimization (Small Files Problem, Partitioning, Z-ORDER, Liquid Clustering)\n",
    "6. **Section 6**: Change Data Feed & Change Data Capture (CDF vs CDC explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782e02f",
   "metadata": {},
   "source": [
    "## Per-user Isolation\n",
    "\n",
    "Run the initialization script for per-user catalog and schema isolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833156cb",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Import libraries and set environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display user context\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
    "    ], ['catalog', 'bronze_schema', 'silver_schema', 'gold_schema'])\n",
    ")\n",
    "\n",
    "# Set catalog and schema as default\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15c88d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Delta Lake Core Features\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake is a transactional layer over Parquet that provides ACID properties (Atomicity, Consistency, Isolation, Durability). Every operation on a Delta table is recorded in the Delta Log - a JSON file containing metadata about changes.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **ACID Transactions**: All operations are atomic and consistent\n",
    "- **Delta Log**: `_delta_log/` folder with JSON files describing each transaction\n",
    "- **Schema Enforcement**: Automatic schema validation - prevents bad data from entering\n",
    "- **Schema Evolution**: Controlled addition of new columns without breaking existing pipelines\n",
    "- **Constraints**: Data quality rules enforced at the table level\n",
    "\n",
    "**Practical Application:**\n",
    "- Transactional updates in Data Lake\n",
    "- Ensuring data quality through schema validation and constraints\n",
    "- Unified data access for batch and streaming workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fade31",
   "metadata": {},
   "source": [
    "### Example 1.1: Creating the First Delta Table\n",
    "\n",
    "**Objective:** Demonstration of creating a Delta table and basic properties\n",
    "\n",
    "**Approach:**\n",
    "1. Load data from Unity Catalog Volume\n",
    "2. Create a managed table in Delta format\n",
    "3. Explore Delta Log and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer data from Unity Catalog Volume\n",
    "customers_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATASET_BASE_PATH}/customers/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db63390",
   "metadata": {},
   "source": [
    "**Create managed Delta table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7de804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create managed Delta table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124d755",
   "metadata": {},
   "source": [
    "**Display result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fbceb4",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "A managed Delta table was created in Unity Catalog. The Delta format automatically:\n",
    "- Created `_delta_log/` folder with transaction metadata\n",
    "- Registered table schema in Unity Catalog\n",
    "- Applied Parquet compression with additional Delta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write customers_df as an external Delta table to a specified path\n",
    "external_path = f\"{DATASET_BASE_PATH}/external/customers_delta\"\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").save(external_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62183b",
   "metadata": {},
   "source": [
    "### Example 1.2: Schema Enforcement in Action\n",
    "\n",
    "**Objective:** Demonstration of automatic schema validation during data insertion\n",
    "\n",
    "Schema Enforcement is a critical feature that prevents \"garbage in, garbage out\" scenarios. Delta Lake compares incoming data schema with the target table schema and **rejects incompatible writes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current table schema\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to insert data with invalid schema (missing columns)\n",
    "invalid_data = spark.createDataFrame([\n",
    "    (\"CUST999999\", \"Test\", \"Customer\", \"invalid_email\", \"+48 123 456 789\")  # Missing city, state, country, registration_date, customer_segment\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\"])\n",
    "\n",
    "try:\n",
    "    invalid_data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "except Exception as e:\n",
    "    display(\n",
    "        spark.createDataFrame([\n",
    "            (\"Schema enforcement in action\", str(e)[:200] + \"...\")\n",
    "        ], [\"message\", \"error\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240eda17",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "Schema enforcement automatically rejected data with incompatible schema. Delta Lake compares the new data schema with the table schema and blocks incompatible insertions, ensuring consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b261c",
   "metadata": {},
   "source": [
    "### Example 1.3: Identity and Generated Columns\n",
    "\n",
    "**Objective:** Demonstrate advanced column features - auto-generated surrogate keys and computed columns\n",
    "\n",
    "Delta Lake supports:\n",
    "- **IDENTITY columns**: Auto-incrementing surrogate keys (unique, increasing, but not contiguous)\n",
    "- **GENERATED columns**: Computed columns derived from other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with Identity Column and Generated Column\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.orders_modern (\n",
    "    order_sk BIGINT GENERATED ALWAYS AS IDENTITY,  -- Surrogate Key\n",
    "    order_id STRING,\n",
    "    total_amount DOUBLE,\n",
    "    order_timestamp TIMESTAMP,\n",
    "    order_date DATE GENERATED ALWAYS AS (CAST(order_timestamp AS DATE)) -- Auto-calculated\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d84232",
   "metadata": {},
   "source": [
    "> **Note:** In a distributed environment like Databricks (Spark/Delta Lake), `GENERATED ALWAYS AS IDENTITY` has specific behaviors. It guarantees **uniqueness** and an **increasing trend**, but does NOT guarantee contiguous numbering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_modern\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d54e4",
   "metadata": {},
   "source": [
    "Now we will insert data. Note that in the `INSERT` query we omit `order_sk` and `order_date` columns:\n",
    "- `order_sk`: will be generated automatically (unique number)\n",
    "- `order_date`: will be calculated based on `order_timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fb8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data without specifying generated columns\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.orders_modern (order_id, total_amount, order_timestamp)\n",
    "VALUES \n",
    "    ('ORD-001', 150.50, current_timestamp()),\n",
    "    ('ORD-002', 200.00, current_timestamp())\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd7256",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(\"orders_modern\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755fdaf",
   "metadata": {},
   "source": [
    "### Example 1.4: Schema Evolution\n",
    "\n",
    "**Objective:** Demonstration of automatic schema evolution when adding new columns\n",
    "\n",
    "Schema Evolution allows for controlled addition of new columns to existing Delta tables without interrupting application operations. Delta Lake supports additive schema changes automatically when enabled with `mergeSchema` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec994af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with additional column (customer_tier)\n",
    "extended_customers = spark.createDataFrame([\n",
    "    (\"CUST010001\", \"New\", \"Customer\", \"new@example.com\", \"+48 111 222 333\", \"Warsaw\", \"MZ\", \"Poland\", \"2023-12-01\", \"Basic\", \"Premium\"),\n",
    "    (\"CUST010002\", \"Another\", \"Customer\", \"another@example.com\", \"+48 444 555 666\", \"Krakow\", \"MP\", \"Poland\", \"2023-12-02\", \"Premium\", \"Standard\")\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\", \"customer_tier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af024c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Cast registration_date to proper type\n",
    "extended_customers = extended_customers.withColumn(\"registration_date\", col(\"registration_date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable automatic schema evolution with mergeSchema option\n",
    "extended_customers.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73575076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new schema - notice the new customer_tier column\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c436018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data - new column has NULL for old records\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"customer_tier\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f608421",
   "metadata": {},
   "source": [
    "### Example 1.5: Data Quality with Constraints\n",
    "\n",
    "**Objective:** Enforce data quality rules at the table level using CHECK constraints\n",
    "\n",
    "Delta Lake allows defining **Constraints** that guarantee data quality at the table level. This works similarly to traditional SQL databases.\n",
    "\n",
    "**Constraint Types:**\n",
    "- `NOT NULL`: Enforces the presence of a value\n",
    "- `CHECK`: Enforces any logical condition (e.g., `age > 0`, `customer_id LIKE 'CUST%'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6adf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CHECK constraint: customer_id must start with CUST\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "        ADD CONSTRAINT valid_customer_id CHECK (customer_id LIKE 'CUST%')\n",
    "    \"\"\")\n",
    "    print(\"Constraint 'valid_customer_id' added successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37177f",
   "metadata": {},
   "source": [
    "Now let's try to insert data that violates the constraint (customer_id does not start with 'CUST').\n",
    "We expect Delta Lake to block this operation and return a `CheckConstraintViolation` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3991ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to insert invalid data (customer_id does not start with CUST)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment)\n",
    "        VALUES ('INVALID123', 'Bad', 'Customer', 'bad@example.com', '+48 000 000 000', 'Test', 'TS', 'Poland', '2023-01-01', 'Basic')\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Expected Data Quality error:\\n{str(e)[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7257a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: CRUD Operations & MERGE\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake supports the full range of CRUD operations (Create, Read, Update, Delete), making it ideal for transactional workloads in Data Lake. All operations are:\n",
    "- **Atomic**: Either fully complete or fully rolled back\n",
    "- **ACID-compliant**: Ensuring data consistency\n",
    "- **Recorded in Delta Log**: Full audit trail of all changes\n",
    "\n",
    "Additionally, Delta Lake provides the powerful **MERGE INTO** operation (also known as \"upsert\") that combines INSERT and UPDATE in a single atomic transaction - essential for CDC (Change Data Capture) scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89e96f",
   "metadata": {},
   "source": [
    "### Example 2.1: INSERT Operation\n",
    "\n",
    "**Objective:** Adding new records to an existing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT new customers\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment, customer_tier)\n",
    "    VALUES \n",
    "        ('CUST020001', 'Insert', 'Customer1', 'insert1@example.com', '+48 111 111 111', 'Warsaw', 'MZ', 'Poland', '2023-12-10', 'Premium', 'Gold'),\n",
    "        ('CUST020002', 'Insert', 'Customer2', 'insert2@example.com', '+48 222 222 222', 'Gdansk', 'PM', 'Poland', '2023-12-11', 'Basic', 'Silver')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7075a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify insertion\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").like(\"CUST02%\"))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd90c38",
   "metadata": {},
   "source": [
    "### Example 2.2: UPDATE Operation\n",
    "\n",
    "**Objective:** Updating existing records in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE customer tier for specific customers\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Platinum'\n",
    "    WHERE customer_id IN ('CUST010001', 'CUST020001')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56de513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify update\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_tier\") == \"Platinum\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3145fe1",
   "metadata": {},
   "source": [
    "### Example 2.3: DELETE Operation\n",
    "\n",
    "**Objective:** Deleting records from a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d834224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE specific customer\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    WHERE customer_id = 'CUST020002'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify deletion\n",
    "deleted_check = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\") \\\n",
    "    .filter(F.col(\"customer_id\") == \"CUST020002\") \\\n",
    "    .count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records with customer_id CUST020002\", deleted_check)\n",
    "    ], [\"description\", \"count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f3d19",
   "metadata": {},
   "source": [
    "### Example 2.4: MERGE INTO (Upsert)\n",
    "\n",
    "**Objective:** Demonstration of upsert operation - update existing and insert new records in a single atomic transaction\n",
    "\n",
    "MERGE INTO is especially useful when processing changes from transactional systems (CDC patterns). It allows you to:\n",
    "- **Update** existing records when a match is found\n",
    "- **Insert** new records when no match exists\n",
    "- **Delete** records based on conditions (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for merge (mix of updates and new records)\n",
    "merge_data = spark.createDataFrame([\n",
    "    (\"CUST010001\", \"Updated\", \"Name\", \"updated@example.com\", \"+48 999 999 999\", \"Poznan\", \"WP\", \"Poland\", \"2023-12-01\", \"VIP\", \"Diamond\"),  # Update existing\n",
    "    (\"CUST030001\", \"Brand\", \"New\", \"brand.new@example.com\", \"+48 777 777 777\", \"Wroclaw\", \"DS\", \"Poland\", \"2023-12-15\", \"Basic\", \"Bronze\"),   # Insert new\n",
    "    (\"CUST030002\", \"Another\", \"New\", \"another.new@example.com\", \"+48 888 888 888\", \"Lodz\", \"LD\", \"Poland\", \"2023-12-16\", \"Premium\", \"Silver\") # Insert new\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\", \"registration_date\", \"customer_segment\", \"customer_tier\"])\n",
    "\n",
    "# Create temporary view for merge operation\n",
    "merge_data.createOrReplaceTempView(\"customer_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a335a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE INTO operation (Upsert)\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta AS target\n",
    "    USING customer_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            first_name = source.first_name,\n",
    "            last_name = source.last_name,\n",
    "            email = source.email,\n",
    "            phone = source.phone,\n",
    "            city = source.city,\n",
    "            state = source.state,\n",
    "            country = source.country,\n",
    "            customer_segment = source.customer_segment,\n",
    "            customer_tier = source.customer_tier\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment, customer_tier)\n",
    "        VALUES (source.customer_id, source.first_name, source.last_name, source.email, source.phone, source.city, source.state, source.country, source.registration_date, source.customer_segment, source.customer_tier)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e666a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify MERGE results\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").isin([\"CUST010001\", \"CUST030001\", \"CUST030002\"]))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1817b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Metadata and Analytics\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Delta Lake offers rich metadata about tables and operations that enables:\n",
    "- **Auditing**: Who changed what and when\n",
    "- **Debugging**: Understanding operation performance and metrics\n",
    "- **Compliance**: Meeting regulatory requirements for data lineage\n",
    "\n",
    "**Key Commands:**\n",
    "- `DESCRIBE DETAIL`: File structure, partitioning, table properties\n",
    "- `DESCRIBE HISTORY`: Complete audit trail of all operations\n",
    "- `SHOW TBLPROPERTIES`: Table configuration and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236758a",
   "metadata": {},
   "source": [
    "### Example 3.1: DESCRIBE DETAIL\n",
    "\n",
    "**Objective:** Analysis of Delta table metadata and physical storage details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed table information\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a2b6f",
   "metadata": {},
   "source": [
    "### Example 3.2: Operation History Analysis\n",
    "\n",
    "**Objective:** Deeper analysis of history and operation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6192936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History with additional metrics\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\", \n",
    "        \"timestamp\", \n",
    "        \"operation\", \n",
    "        \"operationMetrics.numTargetRowsInserted\",\n",
    "        \"operationMetrics.numTargetRowsUpdated\",\n",
    "        \"operationMetrics.numTargetRowsDeleted\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67bb66",
   "metadata": {},
   "source": [
    "### Example 3.3: Delta Log Internals (Deep Dive)\n",
    "\n",
    "**Objective:** Understanding how Delta Lake ensures ACID by looking \"under the hood\" at JSON files in `_delta_log`\n",
    "\n",
    "The Delta Log is a transaction log stored in the `_delta_log/` folder. Each transaction creates a new JSON file containing:\n",
    "- `add`: Adding a new Parquet file with data\n",
    "- `remove`: Logical deletion of a file (e.g., during DELETE or OPTIMIZE)\n",
    "- `commitInfo`: Metadata about the transaction (who, when, what operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table path\n",
    "table_details = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "display(table_details.select(\"location\", \"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View complete Delta table history\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c4e5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Time Travel and Disaster Recovery\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Time Travel is a fundamental Delta Lake feature enabling access to previous versions of data. It is based on the **Copy-on-Write** mechanism - every change creates a new version of files, while old versions remain available until cleaned up by VACUUM.\n",
    "\n",
    "**Key Capabilities:**\n",
    "- **VERSION AS OF**: Query data at a specific version number\n",
    "- **TIMESTAMP AS OF**: Query data at a specific point in time\n",
    "- **RESTORE**: Rollback table to a previous state\n",
    "- **Audit**: Compare data between versions\n",
    "\n",
    "**Important Consideration:** VACUUM removes old files and directly impacts Time Travel capabilities. Understanding this relationship is crucial for data retention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2d244",
   "metadata": {},
   "source": [
    "### Example 4.1: Creating a Dedicated Table for Time Travel Demo\n",
    "\n",
    "**Objective:** Create a fresh table to demonstrate Time Travel features clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table specifically for Time Travel demonstration\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    status STRING,\n",
    "    updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Version 0: Insert initial data\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VALUES\n",
    "    (1, 'Alice', 'active', current_timestamp()),\n",
    "    (2, 'Bob', 'active', current_timestamp()),\n",
    "    (3, 'Charlie', 'active', current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 0: Initial data inserted\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caaa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Update some records\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\n",
    "SET status = 'premium', updated_at = current_timestamp()\n",
    "WHERE name = 'Alice'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 1: Alice upgraded to premium\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: Insert new record\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VALUES\n",
    "    (4, 'Diana', 'new', current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 2: Diana added\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: Delete a record\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\n",
    "WHERE name = 'Charlie'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Version 3: Charlie deleted\")\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73b46f",
   "metadata": {},
   "source": [
    "### Example 4.2: Table History Exploration\n",
    "\n",
    "**Objective:** Use DESCRIBE HISTORY to analyze all operations on the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce0780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show complete history of all operations\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4b5c0",
   "metadata": {},
   "source": [
    "### Example 4.3: Time Travel Queries\n",
    "\n",
    "**Objective:** Access previous versions of data using VERSION AS OF and TIMESTAMP AS OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e681f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data from version 0 (initial state)\n",
    "print(\"Version 0 - Initial data (before any changes):\")\n",
    "display(\n",
    "    spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF 0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ddd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data from version 1 (after Alice upgrade)\n",
    "print(\"Version 1 - After Alice upgrade:\")\n",
    "display(\n",
    "    spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF 1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare record counts between versions\n",
    "version_counts = []\n",
    "for v in range(4):\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF {v}\").first()[0]\n",
    "    version_counts.append((f\"Version {v}\", count))\n",
    "\n",
    "display(spark.createDataFrame(version_counts, [\"version\", \"record_count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb363390",
   "metadata": {},
   "source": [
    "### Example 4.4: Disaster Recovery - Accidental Deletion\n",
    "\n",
    "**Objective:** Simulate accidental data deletion and recover using RESTORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISASTER! Accidental deletion of ALL data\n",
    "spark.sql(f\"DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "\n",
    "print(\"Oh no! All data deleted!\")\n",
    "print(\"Record count after deletion:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eedf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check history to find the last good version\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTORE to version before the accidental deletion\n",
    "# The last good version is the one before DELETE (version 3 in our case)\n",
    "last_good_version = spark.sql(f\"\"\"\n",
    "    SELECT version FROM (\n",
    "        SELECT version, operation, \n",
    "               ROW_NUMBER() OVER (ORDER BY version DESC) as rn\n",
    "        FROM (DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo)\n",
    "        WHERE operation != 'DELETE'\n",
    "    ) WHERE rn = 1\n",
    "\"\"\").first()[0]\n",
    "\n",
    "print(f\"Restoring to version: {last_good_version}\")\n",
    "spark.sql(f\"RESTORE TABLE {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo TO VERSION AS OF {last_good_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34927fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify restoration\n",
    "print(\"Data restored successfully!\")\n",
    "print(\"Record count after RESTORE:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\").count())\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbce6dd",
   "metadata": {},
   "source": [
    "### Example 4.5: VACUUM and Its Impact on Time Travel\n",
    "\n",
    "**Objective:** Understand how VACUUM affects Time Travel capabilities\n",
    "\n",
    "**Critical Concept:** VACUUM removes old data files that are no longer referenced by the current version of the table. Once vacuumed, **Time Travel to those versions becomes impossible**.\n",
    "\n",
    "**Default Retention:** 7 days (168 hours)\n",
    "- This means you can Time Travel to any version within the last 7 days\n",
    "- After VACUUM, only versions within the retention period are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current table size and files BEFORE VACUUM\n",
    "print(\"=== BEFORE VACUUM ===\")\n",
    "before_vacuum = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "display(before_vacuum.select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf11130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what versions are available BEFORE vacuum\n",
    "print(\"Available versions before VACUUM:\")\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\").select(\"version\", \"timestamp\", \"operation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACUUM with 0 hours retention (DEMO ONLY - requires disabling safety check)\n",
    "# In production, NEVER use 0 hours - use default 7 days or more!\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "\n",
    "vacuum_result = spark.sql(f\"\"\"\n",
    "    VACUUM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo RETAIN 0 HOURS\n",
    "\"\"\")\n",
    "\n",
    "display(vacuum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table size AFTER VACUUM\n",
    "print(\"=== AFTER VACUUM ===\")\n",
    "after_vacuum = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo\")\n",
    "display(after_vacuum.select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb38989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to access an old version - this will fail!\n",
    "try:\n",
    "    spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.time_travel_demo VERSION AS OF 0\").show()\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Time Travel failed after VACUUM!\")\n",
    "    print(f\"Error: {str(e)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0faa8",
   "metadata": {},
   "source": [
    "**Key Takeaway:** \n",
    "- VACUUM is essential for storage optimization (removes orphaned files)\n",
    "- But it **permanently destroys** the ability to Time Travel to vacuumed versions\n",
    "- Always set appropriate retention period based on your recovery requirements\n",
    "- Default 7 days is a good balance for most use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812bdf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Optimization\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "As data grows, query performance can degrade due to several factors:\n",
    "- **Small Files Problem**: Too many small files increase metadata overhead\n",
    "- **Data Layout**: Data not organized for common query patterns\n",
    "- **Predicate Pushdown Inefficiency**: Scanning more data than necessary\n",
    "\n",
    "Delta Lake provides several optimization techniques:\n",
    "\n",
    "| Technique | Description | When to Use |\n",
    "|-----------|-------------|-------------|\n",
    "| **OPTIMIZE** | Compacts small files into larger ones | After many small writes |\n",
    "| **Partitioning** | Physical data separation by column values | High-cardinality filter columns |\n",
    "| **Z-ORDER** | Co-locates related data for better pruning | Frequently filtered columns |\n",
    "| **Liquid Clustering** | Modern alternative to partitioning + Z-ORDER | New tables (recommended) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbb7ab",
   "metadata": {},
   "source": [
    "### Example 5.1: The Small Files Problem\n",
    "\n",
    "**Objective:** Demonstrate how many small files impact performance and how OPTIMIZE solves it\n",
    "\n",
    "The \"small files problem\" occurs when:\n",
    "- Streaming jobs write many small files\n",
    "- Frequent small batch inserts\n",
    "- High-concurrency writes\n",
    "\n",
    "This leads to:\n",
    "- Increased metadata overhead\n",
    "- Slower query performance\n",
    "- Higher storage costs (metadata per file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e68772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with many small files (simulating streaming ingestion)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.small_files_demo (\n",
    "    id INT,\n",
    "    data STRING,\n",
    "    created_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Insert data in many small batches (simulating streaming)\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import random\n",
    "import string\n",
    "\n",
    "print(\"Inserting 500 small batches to simulate streaming ingestion...\")\n",
    "\n",
    "for i in range(500):\n",
    "    # Each batch has only 10-50 records\n",
    "    batch_size = random.randint(10, 50)\n",
    "    data = [(j, ''.join(random.choices(string.ascii_letters, k=20)), None) \n",
    "            for j in range(batch_size)]\n",
    "    df = spark.createDataFrame(data, [\"id\", \"data\", \"created_at\"]) \\\n",
    "        .withColumn(\"created_at\", current_timestamp())\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
    "\n",
    "print(\"Done! 500 small batches inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1550c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of files BEFORE optimization\n",
    "print(\"=== BEFORE OPTIMIZE ===\")\n",
    "before_optimize = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
    "display(before_optimize.select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e2595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OPTIMIZE to compact small files\n",
    "optimize_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c490026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of files AFTER optimization\n",
    "print(\"=== AFTER OPTIMIZE ===\")\n",
    "after_optimize = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.small_files_demo\")\n",
    "display(after_optimize.select(\"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd1d3f",
   "metadata": {},
   "source": [
    "### Example 5.2: Partitioning\n",
    "\n",
    "**Objective:** Demonstrate how partitioning improves query performance for filtered queries\n",
    "\n",
    "Partitioning physically separates data into directories based on column values. This enables:\n",
    "- **Partition Pruning**: Skip entire partitions that don't match the filter\n",
    "- **Parallel Processing**: Process partitions independently\n",
    "- **Efficient Deletes/Updates**: Only touch affected partitions\n",
    "\n",
    "**Best Practices:**\n",
    "- Use low-cardinality columns (e.g., date, country, status)\n",
    "- Avoid over-partitioning (too many small partitions)\n",
    "- Consider partition size: aim for 1GB+ per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe42ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partitioned table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned (\n",
    "    order_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    order_date DATE,\n",
    "    amount DOUBLE,\n",
    "    status STRING\n",
    ") \n",
    "USING DELTA\n",
    "PARTITIONED BY (order_date)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample data across multiple dates\n",
    "from datetime import date, timedelta\n",
    "\n",
    "orders_data = []\n",
    "base_date = date(2024, 1, 1)\n",
    "\n",
    "for day_offset in range(30):  # 30 days of data\n",
    "    order_date = base_date + timedelta(days=day_offset)\n",
    "    for i in range(100):  # 100 orders per day\n",
    "        orders_data.append((\n",
    "            f\"ORD-{day_offset:02d}-{i:04d}\",\n",
    "            f\"CUST{i % 50:04d}\",\n",
    "            f\"PROD{i % 20:03d}\",\n",
    "            order_date,\n",
    "            round(50 + (i * 2.5), 2),\n",
    "            \"completed\" if i % 3 != 0 else \"pending\"\n",
    "        ))\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, \n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"amount\", \"status\"])\n",
    "\n",
    "orders_df.write.format(\"delta\").mode(\"append\").partitionBy(\"order_date\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\")\n",
    "\n",
    "print(f\"Inserted {len(orders_data)} orders across 30 days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95503875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check partitioning structure\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ac51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with partition filter - only scans relevant partitions\n",
    "# Check the Spark UI to see partition pruning in action\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.orders_partitioned\n",
    "    WHERE order_date = '2024-01-15'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Query for single date (should scan only 1 partition):\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5940438",
   "metadata": {},
   "source": [
    "### Example 5.3: Z-ORDER (Data Skipping)\n",
    "\n",
    "**Objective:** Demonstrate how Z-ORDER improves query performance by co-locating related data\n",
    "\n",
    "Z-ORDER is a multi-dimensional clustering technique that:\n",
    "- Co-locates related data in the same files\n",
    "- Enables efficient data skipping based on file-level statistics\n",
    "- Works best with high-cardinality columns used in filters\n",
    "\n",
    "**When to use Z-ORDER:**\n",
    "- Columns frequently used in WHERE clauses\n",
    "- Columns with high cardinality\n",
    "- Can specify up to 4 columns (effectiveness decreases with more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table for Z-ORDER demonstration\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo (\n",
    "    sale_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    store_id STRING,\n",
    "    sale_date DATE,\n",
    "    amount DOUBLE,\n",
    "    quantity INT\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample data\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "sales_data = []\n",
    "for i in range(100000):  # 100K records\n",
    "    sales_data.append((\n",
    "        f\"SALE-{i:08d}\",\n",
    "        f\"CUST{random.randint(1, 1000):04d}\",\n",
    "        f\"PROD{random.randint(1, 500):03d}\",\n",
    "        f\"STORE{random.randint(1, 50):02d}\",\n",
    "        date(2024, random.randint(1, 12), random.randint(1, 28)),\n",
    "        round(random.uniform(10, 500), 2),\n",
    "        random.randint(1, 10)\n",
    "    ))\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, \n",
    "    [\"sale_id\", \"customer_id\", \"product_id\", \"store_id\", \"sale_date\", \"amount\", \"quantity\"])\n",
    "\n",
    "sales_df.write.format(\"delta\").mode(\"append\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\")\n",
    "\n",
    "print(\"Inserted 100,000 sales records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file statistics BEFORE Z-ORDER\n",
    "print(\"=== BEFORE Z-ORDER ===\")\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Z-ORDER on frequently filtered columns\n",
    "# In this case: customer_id and product_id are common filter columns\n",
    "zorder_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
    "    ZORDER BY (customer_id, product_id)\n",
    "\"\"\")\n",
    "\n",
    "display(zorder_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now queries filtering by customer_id or product_id will skip more files\n",
    "# Example query that benefits from Z-ORDER\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.sales_zorder_demo\n",
    "    WHERE customer_id = 'CUST0042' AND product_id = 'PROD123'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Query with Z-ORDER optimized columns (check Spark UI for data skipping):\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e97918",
   "metadata": {},
   "source": [
    "### Example 5.4: Liquid Clustering (Modern Approach)\n",
    "\n",
    "**Objective:** Introduce Liquid Clustering as a modern replacement for partitioning and Z-ORDER\n",
    "\n",
    "**Liquid Clustering** is Databricks' newest optimization technique that:\n",
    "- Automatically manages data layout\n",
    "- Adapts to changing query patterns\n",
    "- Eliminates need for manual partitioning decisions\n",
    "- Works incrementally (no need to re-cluster entire table)\n",
    "\n",
    "**Key Benefits:**\n",
    "- No upfront partitioning decisions required\n",
    "- Can change clustering columns without rewriting data\n",
    "- Better performance for evolving workloads\n",
    "- Simpler to manage than partitioning + Z-ORDER combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with Liquid Clustering\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering (\n",
    "    sale_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    region STRING,\n",
    "    sale_date DATE,\n",
    "    amount DOUBLE,\n",
    "    quantity INT\n",
    ") \n",
    "USING DELTA\n",
    "CLUSTER BY (customer_id, region)  -- Liquid Clustering columns\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02118b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample data\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "\n",
    "sales_data = []\n",
    "for i in range(50000):\n",
    "    sales_data.append((\n",
    "        f\"SALE-{i:08d}\",\n",
    "        f\"CUST{random.randint(1, 500):04d}\",\n",
    "        f\"PROD{random.randint(1, 200):03d}\",\n",
    "        random.choice(regions),\n",
    "        date(2024, random.randint(1, 12), random.randint(1, 28)),\n",
    "        round(random.uniform(10, 500), 2),\n",
    "        random.randint(1, 10)\n",
    "    ))\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, \n",
    "    [\"sale_id\", \"customer_id\", \"product_id\", \"region\", \"sale_date\", \"amount\", \"quantity\"])\n",
    "\n",
    "sales_df.write.format(\"delta\").mode(\"append\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\")\n",
    "\n",
    "print(\"Inserted 50,000 sales records into Liquid Clustering table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE automatically applies Liquid Clustering\n",
    "# No need to specify ZORDER - it's built into the table definition!\n",
    "optimize_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a599078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check clustering information\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries filtering by clustering columns are automatically optimized\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT region, COUNT(*) as sales_count, SUM(amount) as total_amount\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.sales_liquid_clustering\n",
    "    WHERE customer_id LIKE 'CUST00%' AND region = 'North'\n",
    "    GROUP BY region\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91f3bc",
   "metadata": {},
   "source": [
    "**Comparison: Partitioning vs Z-ORDER vs Liquid Clustering**\n",
    "\n",
    "| Feature | Partitioning | Z-ORDER | Liquid Clustering |\n",
    "|---------|-------------|---------|-------------------|\n",
    "| When to choose | Low-cardinality columns | High-cardinality filter columns | General purpose (recommended) |\n",
    "| Data layout | Directory per partition | Co-located in files | Automatic clustering |\n",
    "| Schema change | Requires rewrite | Easy to change | Easy to change |\n",
    "| Maintenance | Manual | Manual OPTIMIZE | Automatic with OPTIMIZE |\n",
    "| Best for | Date/Region filters | Multi-column filters | Evolving workloads |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de76ae7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Change Data Feed vs Change Data Capture\n",
    "\n",
    "**Theoretical Introduction:**\n",
    "\n",
    "Two terms are often confused in the data engineering world: **Change Data Feed (CDF)** and **Change Data Capture (CDC)**. Understanding the difference is crucial:\n",
    "\n",
    "### Change Data Capture (CDC)\n",
    "**What it is:** A *pattern/technique* for capturing changes from source systems (databases, APIs, etc.)\n",
    "\n",
    "**Characteristics:**\n",
    "- Source-side technology\n",
    "- Captures INSERT, UPDATE, DELETE from operational databases\n",
    "- Tools: Debezium, AWS DMS, Fivetran, Qlik Replicate\n",
    "- Produces a stream of change events\n",
    "\n",
    "**Example:** Capturing changes from PostgreSQL and streaming them to Kafka\n",
    "\n",
    "### Change Data Feed (CDF)\n",
    "**What it is:** A *Delta Lake feature* that records row-level changes within Delta tables\n",
    "\n",
    "**Characteristics:**\n",
    "- Delta Lake native feature\n",
    "- Tracks changes that happen WITHIN Delta tables\n",
    "- Provides `_change_type`, `_commit_version`, `_commit_timestamp` columns\n",
    "- Enables efficient incremental processing\n",
    "\n",
    "**Example:** Reading only the rows that changed since the last pipeline run\n",
    "\n",
    "### How They Work Together\n",
    "```\n",
    "[Source DB] --CDC--> [Bronze Delta] --CDF--> [Silver Delta] --CDF--> [Gold Delta]\n",
    "     ^                    ^                      ^                       ^\n",
    "     |                    |                      |                       |\n",
    "   CDC captures      CDF tracks            CDF tracks              CDF tracks\n",
    "   source changes    Bronze changes        Silver changes          Gold changes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd725d36",
   "metadata": {},
   "source": [
    "### Example 6.1: Enabling Change Data Feed\n",
    "\n",
    "**Objective:** Enable CDF on a Delta table and understand what metadata is captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8de331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with CDF enabled from the start\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{BRONZE_SCHEMA}.cdf_demo (\n",
    "    user_id STRING,\n",
    "    name STRING,\n",
    "    email STRING,\n",
    "    status STRING,\n",
    "    updated_at TIMESTAMP\n",
    ") \n",
    "USING DELTA\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created with Change Data Feed enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CDF is enabled\n",
    "properties = spark.sql(f\"SHOW TBLPROPERTIES {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
    "display(properties.filter(F.col(\"key\").like(\"%change%\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12269eba",
   "metadata": {},
   "source": [
    "### Example 6.2: Generating and Tracking Changes\n",
    "\n",
    "**Objective:** Execute various DML operations and see how CDF records them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT some initial data\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.cdf_demo VALUES\n",
    "    ('U001', 'Alice', 'alice@example.com', 'active', current_timestamp()),\n",
    "    ('U002', 'Bob', 'bob@example.com', 'active', current_timestamp()),\n",
    "    ('U003', 'Charlie', 'charlie@example.com', 'active', current_timestamp())\n",
    "\"\"\")\n",
    "print(\"Version 1: Initial INSERT completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE a record\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\n",
    "SET status = 'premium', updated_at = current_timestamp()\n",
    "WHERE user_id = 'U001'\n",
    "\"\"\")\n",
    "print(\"Version 2: UPDATE completed - Alice upgraded to premium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE a record\n",
    "spark.sql(f\"\"\"\n",
    "DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.cdf_demo\n",
    "WHERE user_id = 'U002'\n",
    "\"\"\")\n",
    "print(\"Version 3: DELETE completed - Bob removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29690bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT new record\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.cdf_demo VALUES\n",
    "    ('U004', 'Diana', 'diana@example.com', 'trial', current_timestamp())\n",
    "\"\"\")\n",
    "print(\"Version 4: INSERT completed - Diana added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd89de7",
   "metadata": {},
   "source": [
    "### Example 6.3: Reading Change Data Feed\n",
    "\n",
    "**Objective:** Query the Change Data Feed to see all recorded changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560518b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all changes from the beginning\n",
    "changes = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
    "\n",
    "# Show changes with CDF metadata columns\n",
    "display(\n",
    "    changes.select(\n",
    "        \"user_id\", \"name\", \"status\",\n",
    "        \"_change_type\",        # insert, update_preimage, update_postimage, delete\n",
    "        \"_commit_version\",     # Delta version number\n",
    "        \"_commit_timestamp\"    # When the change occurred\n",
    "    ).orderBy(\"_commit_version\", \"user_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e5626",
   "metadata": {},
   "source": [
    "**Understanding `_change_type` values:**\n",
    "\n",
    "| Change Type | Description |\n",
    "|-------------|-------------|\n",
    "| `insert` | New row inserted |\n",
    "| `update_preimage` | Row value BEFORE update |\n",
    "| `update_postimage` | Row value AFTER update |\n",
    "| `delete` | Row that was deleted |\n",
    "\n",
    "This enables powerful incremental processing patterns - you can process only what changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get only new inserts since version 2\n",
    "new_inserts = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 2) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\") \\\n",
    "    .filter(F.col(\"_change_type\") == \"insert\")\n",
    "\n",
    "print(\"New inserts since version 2:\")\n",
    "display(new_inserts.select(\"user_id\", \"name\", \"status\", \"_commit_version\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get all deletions for audit purposes\n",
    "deletions = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\") \\\n",
    "    .filter(F.col(\"_change_type\") == \"delete\")\n",
    "\n",
    "print(\"All deleted records (for audit):\")\n",
    "display(deletions.select(\"user_id\", \"name\", \"_commit_version\", \"_commit_timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77450091",
   "metadata": {},
   "source": [
    "### Example 6.4: CDF for Incremental ETL\n",
    "\n",
    "**Objective:** Demonstrate how CDF enables efficient incremental processing in ETL pipelines\n",
    "\n",
    "Instead of reprocessing entire tables, use CDF to process only changed rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an incremental ETL pipeline\n",
    "# First run: Process all data (startingVersion = 0)\n",
    "# Subsequent runs: Process only changes since last processed version\n",
    "\n",
    "# Store the last processed version (in practice, save this to a checkpoint table)\n",
    "last_processed_version = 0\n",
    "\n",
    "# Read incremental changes\n",
    "incremental_changes = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", last_processed_version) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.cdf_demo\")\n",
    "\n",
    "# Apply transformations only to changed records\n",
    "transformed = incremental_changes \\\n",
    "    .filter(F.col(\"_change_type\").isin([\"insert\", \"update_postimage\"])) \\\n",
    "    .withColumn(\"processed_at\", F.current_timestamp()) \\\n",
    "    .withColumn(\"email_domain\", F.split(F.col(\"email\"), \"@\")[1])\n",
    "\n",
    "print(\"Incremental processing - only changed records:\")\n",
    "display(transformed.select(\"user_id\", \"name\", \"email_domain\", \"status\", \"_change_type\", \"processed_at\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9b3b1",
   "metadata": {},
   "source": [
    "**Key Takeaways - CDF vs CDC:**\n",
    "\n",
    "1. **CDC** captures changes FROM source systems INTO your lakehouse\n",
    "2. **CDF** tracks changes WITHIN Delta Lake tables\n",
    "3. Use CDC tools (Debezium, DMS) to ingest data into Bronze\n",
    "4. Use CDF to build efficient incremental Silver and Gold layers\n",
    "5. CDF eliminates need for expensive full-table scans in pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541ab29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What Has Been Achieved:\n",
    "\n",
    "| Section | Key Learnings |\n",
    "|---------|--------------|\n",
    "| **Section 1** | Delta table creation, Schema Enforcement, Schema Evolution, Constraints |\n",
    "| **Section 2** | Complete CRUD operations, MERGE INTO for upserts |\n",
    "| **Section 3** | Metadata exploration with DESCRIBE DETAIL/HISTORY, Delta Log internals |\n",
    "| **Section 4** | Time Travel queries, Disaster Recovery with RESTORE, VACUUM implications |\n",
    "| **Section 5** | Small Files Problem, Partitioning, Z-ORDER, Liquid Clustering |\n",
    "| **Section 6** | Change Data Feed vs CDC - understanding the difference |\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Delta Lake = Data Lake + ACID**: Combines Data Lake flexibility with transactional reliability\n",
    "2. **Schema Evolution safely**: Additive changes are automatic, breaking changes require planning\n",
    "3. **Time Travel + Copy-on-Write**: Every version is preserved, enabling rollback and audit\n",
    "4. **VACUUM trade-off**: Storage optimization vs Time Travel capability\n",
    "5. **Optimization matters**: Choose the right technique (Partitioning, Z-ORDER, Liquid Clustering)\n",
    "6. **CDF ≠ CDC**: CDC ingests from sources, CDF tracks Delta Lake changes\n",
    "\n",
    "### Quick Reference - Key Commands:\n",
    "\n",
    "| Operation | SQL | PySpark |\n",
    "|-----------|-----|---------|\n",
    "| Create Delta Table | `CREATE TABLE USING DELTA` | `df.write.format(\"delta\").saveAsTable()` |\n",
    "| Time Travel | `SELECT * FROM table VERSION AS OF 1` | `.option(\"versionAsOf\", 1)` |\n",
    "| Restore | `RESTORE TABLE table TO VERSION AS OF 1` | N/A |\n",
    "| MERGE | `MERGE INTO target USING source` | `DeltaTable.forName().merge()` |\n",
    "| Optimize | `OPTIMIZE table` | N/A |\n",
    "| Z-ORDER | `OPTIMIZE table ZORDER BY (col)` | N/A |\n",
    "| VACUUM | `VACUUM table RETAIN X HOURS` | N/A |\n",
    "| History | `DESCRIBE HISTORY table` | N/A |\n",
    "| Enable CDF | `ALTER TABLE SET TBLPROPERTIES (delta.enableChangeDataFeed = true)` | N/A |\n",
    "| Read CDF | N/A | `.option(\"readChangeFeed\", \"true\")` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257fa9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resource Cleanup\n",
    "\n",
    "Clean up resources created during the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional test resource cleanup\n",
    "# NOTE: Run only if you want to delete all created data\n",
    "\n",
    "# Tables to clean up:\n",
    "cleanup_tables = [\n",
    "    \"customers_delta\",\n",
    "    \"orders_modern\", \n",
    "    \"time_travel_demo\",\n",
    "    \"small_files_demo\",\n",
    "    \"orders_partitioned\",\n",
    "    \"sales_zorder_demo\",\n",
    "    \"sales_liquid_clustering\",\n",
    "    \"cdf_demo\"\n",
    "]\n",
    "\n",
    "# Uncomment below to execute cleanup:\n",
    "# for table in cleanup_tables:\n",
    "#     spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.{table}\")\n",
    "#     print(f\"Dropped: {table}\")\n",
    "\n",
    "# spark.sql(\"DROP VIEW IF EXISTS customer_updates\")\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# print(\"All resources cleaned up!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
